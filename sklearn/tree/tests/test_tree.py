"""
Testing for the tree module (sklearn.tree).
"""

import copy
import copyreg
import io
import pickle
import struct
from itertools import chain, product

import joblib
import numpy as np
import pytest
from joblib.numpy_pickle import NumpyPickler
from numpy.testing import assert_allclose

from sklearn import clone, datasets, tree
from sklearn.dummy import DummyRegressor
from sklearn.exceptions import NotFittedError
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, mean_poisson_deviance, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.random_projection import _sparse_random_matrix
from sklearn.tree import (
    DecisionTreeClassifier,
    DecisionTreeRegressor,
    ExtraTreeClassifier,
    ExtraTreeRegressor,
    ExpectedValueDecisionTreeRegressor,
)
from sklearn.tree._classes import (
    CRITERIA_CLF,
    CRITERIA_REG,
    DENSE_SPLITTERS,
    SPARSE_SPLITTERS,
)
from sklearn.tree._tree import (
    NODE_DTYPE,
    TREE_LEAF,
    TREE_UNDEFINED,
    _check_n_classes,
    _check_node_ndarray,
    _check_value_ndarray,
)
from sklearn.tree._tree import Tree as CythonTree
from sklearn.utils import compute_sample_weight
from sklearn.utils._testing import (
    assert_almost_equal,
    assert_array_almost_equal,
    assert_array_equal,
    create_memmap_backed_data,
    ignore_warnings,
    skip_if_32bit,
)
from sklearn.utils.estimator_checks import check_sample_weights_invariance
from sklearn.utils.fixes import (
    _IS_32BIT,
    COO_CONTAINERS,
    CSC_CONTAINERS,
    CSR_CONTAINERS,
)
from sklearn.utils.validation import check_random_state

CLF_CRITERIONS = ("gini", "log_loss")
REG_CRITERIONS = ("squared_error", "absolute_error", "friedman_mse", "poisson")

CLF_TREES = {
    "DecisionTreeClassifier": DecisionTreeClassifier,
    "ExtraTreeClassifier": ExtraTreeClassifier,
}

REG_TREES = {
    "DecisionTreeRegressor": DecisionTreeRegressor,
    "ExtraTreeRegressor": ExtraTreeRegressor,
}

ALL_TREES: dict = dict()
ALL_TREES.update(CLF_TREES)
ALL_TREES.update(REG_TREES)

SPARSE_TREES = [
    "DecisionTreeClassifier",
    "DecisionTreeRegressor",
    "ExtraTreeClassifier",
    "ExtraTreeRegressor",
]


X_small = np.array(
    [
        [0, 0, 4, 0, 0, 0, 1, -14, 0, -4, 0, 0, 0, 0],
        [0, 0, 5, 3, 0, -4, 0, 0, 1, -5, 0.2, 0, 4, 1],
        [-1, -1, 0, 0, -4.5, 0, 0, 2.1, 1, 0, 0, -4.5, 0, 1],
        [-1, -1, 0, -1.2, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 1],
        [-1, -1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1],
        [-1, -2, 0, 4, -3, 10, 4, 0, -3.2, 0, 4, 3, -4, 1],
        [2.11, 0, -6, -0.5, 0, 11, 0, 0, -3.2, 6, 0.5, 0, -3, 1],
        [2.11, 0, -6, -0.5, 0, 11, 0, 0, -3.2, 6, 0, 0, -2, 1],
        [2.11, 8, -6, -0.5, 0, 11, 0, 0, -3.2, 6, 0, 0, -2, 1],
        [2.11, 8, -6, -0.5, 0, 11, 0, 0, -3.2, 6, 0.5, 0, -1, 0],
        [2, 8, 5, 1, 0.5, -4, 10, 0, 1, -5, 3, 0, 2, 0],
        [2, 0, 1, 1, 1, -1, 1, 0, 0, -2, 3, 0, 1, 0],
        [2, 0, 1, 2, 3, -1, 10, 2, 0, -1, 1, 2, 2, 0],
        [1, 1, 0, 2, 2, -1, 1, 2, 0, -5, 1, 2, 3, 0],
        [3, 1, 0, 3, 0, -4, 10, 0, 1, -5, 3, 0, 3, 1],
        [2.11, 8, -6, -0.5, 0, 1, 0, 0, -3.2, 6, 0.5, 0, -3, 1],
        [2.11, 8, -6, -0.5, 0, 1, 0, 0, -3.2, 6, 1.5, 1, -1, -1],
        [2.11, 8, -6, -0.5, 0, 10, 0, 0, -3.2, 6, 0.5, 0, -1, -1],
        [2, 0, 5, 1, 0.5, -2, 10, 0, 1, -5, 3, 1, 0, -1],
        [2, 0, 1, 1, 1, -2, 1, 0, 0, -2, 0, 0, 0, 1],
        [2, 1, 1, 1, 2, -1, 10, 2, 0, -1, 0, 2, 1, 1],
        [1, 1, 0, 0, 1, -3, 1, 2, 0, -5, 1, 2, 1, 1],
        [3, 1, 0, 1, 0, -4, 1, 0, 1, -2, 0, 0, 1, 0],
    ]
)

y_small = [1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]
y_small_reg = [
    1.0,
    2.1,
    1.2,
    0.05,
    10,
    2.4,
    3.1,
    1.01,
    0.01,
    2.98,
    3.1,
    1.1,
    0.0,
    1.2,
    2,
    11,
    0,
    0,
    4.5,
    0.201,
    1.06,
    0.9,
    0,
]

prices_value = np.array([ 0.        ,  0.2020202 ,  0.4040404 ,  0.60606061,  0.80808081,
        1.01010101,  1.21212121,  1.41414141,  1.61616162,  1.81818182,
        2.02020202,  2.22222222,  2.42424242,  2.62626263,  2.82828283,
        3.03030303,  3.23232323,  3.43434343,  3.63636364,  3.83838384,
        4.04040404,  4.24242424,  4.44444444,  4.64646465,  4.84848485,
        5.05050505,  5.25252525,  5.45454545,  5.65656566,  5.85858586,
        6.06060606,  6.26262626,  6.46464646,  6.66666667,  6.86868687,
        7.07070707,  7.27272727,  7.47474747,  7.67676768,  7.87878788,
        8.08080808,  8.28282828,  8.48484848,  8.68686869,  8.88888889,
        9.09090909,  9.29292929,  9.49494949,  9.6969697 ,  9.8989899 ,
       10.1010101 , 10.3030303 , 10.50505051, 10.70707071, 10.90909091,
       11.11111111, 11.31313131, 11.51515152, 11.71717172, 11.91919192,
       12.12121212, 12.32323232, 12.52525253, 12.72727273, 12.92929293,
       13.13131313, 13.33333333, 13.53535354, 13.73737374, 13.93939394,
       14.14141414, 14.34343434, 14.54545455, 14.74747475, 14.94949495,
       15.15151515, 15.35353535, 15.55555556, 15.75757576, 15.95959596,
       16.16161616, 16.36363636, 16.56565657, 16.76767677, 16.96969697,
       17.17171717, 17.37373737, 17.57575758, 17.77777778, 17.97979798,
       18.18181818, 18.38383838, 18.58585859, 18.78787879, 18.98989899,
       19.19191919, 19.39393939, 19.5959596 , 19.7979798 , 20.        ])

deal_value = np.array([[      0.        ,   45260.05384478,   90520.10768956,
         135780.16153434,  181040.21537912,  226300.2692239 ,
         271560.32306867,  316820.37691345,  362080.43075823,
         407340.48460301,  452600.53844779,  497860.59229257,
         543120.64613735,  588380.69998213,  633640.75382691,
         678900.80767169,  724160.86151647,  769420.91536125,
         814680.96920602,  859941.0230508 ,  905201.07689558,
         950461.13074036,  995721.18458514, 1040981.23842992,
        1086241.2922747 , 1131501.34611948, 1176761.39996426,
        1222021.45380904, 1267281.50765382, 1312541.56149859,
        1357801.61534337, 1403061.66918815, 1448321.72303293,
        1493581.77687771, 1538841.83072249, 1584101.88456727,
        1629361.93841205, 1674621.99225683, 1719882.04610161,
        1765142.09994639, 1810402.15379116, 1855662.20763594,
        1900922.26148072, 1946182.3153255 , 1991442.36917028,
        2036702.42301506, 2081962.47685984, 2127222.53070462,
        2172482.5845494 , 2217742.63839418, 2263002.69223896,
        2308262.74608374, 2353522.79992851, 2398782.85377329,
        2444042.90761807, 2489302.96146285, 2534563.01530763,
        2579823.06915241, 2625083.12299719, 2670343.17684197,
        2715603.23068675, 2760863.28453153, 2806123.33837631,
        2851383.39222108, 2896643.44606586, 2941903.49991064,
        2987163.55375542, 3032423.6076002 , 3077683.66144498,
        3122943.71528976, 3168203.76913454, 3213463.82297932,
        3258723.8768241 , 3303983.93066888, 3349243.98451365,
        3394504.03835843, 3439764.09220321, 3485024.14604799,
        3530284.19989277, 3575544.25373755, 3620804.30758233,
        3666064.36142711, 3711324.41527189, 3756584.46911667,
        3801844.52296145, 3847104.57680623, 3892364.630651  ,
        3937624.68449578, 3982884.73834056, 4028144.79218534,
        4073404.84603012, 4118664.8998749 , 4163924.95371968,
        4209185.00756446, 4254445.06140924, 4299705.11525402,
        4344965.1690988 , 4390225.22294357, 4435485.27678835,
        4480745.33063313],
       [      0.        ,   58368.45512833,  116736.91025666,
         175105.365385  ,  233473.82051333,  291842.27564166,
         350210.73076999,  408579.18589832,  466947.64102666,
         525316.09615499,  583684.55128332,  642053.00641165,
         700421.46153998,  758789.91666832,  817158.37179665,
         875526.82692498,  933895.28205331,  992263.73718164,
        1050632.19230998, 1109000.64743831, 1167369.10256664,
        1225737.55769497, 1284106.0128233 , 1342474.46795164,
        1400842.92307997, 1459211.3782083 , 1517579.83333663,
        1575948.28846497, 1634316.7435933 , 1692685.19872163,
        1751053.65384996, 1809422.10897829, 1867790.56410663,
        1926159.01923496, 1984527.47436329, 2042895.92949162,
        2101264.38461995, 2159632.83974829, 2218001.29487662,
        2276369.75000495, 2334738.20513328, 2393106.66026161,
        2451475.11538995, 2509843.57051828, 2568212.02564661,
        2626580.48077494, 2684948.93590327, 2743317.39103161,
        2801685.84615994, 2860054.30128827, 2918422.7564166 ,
        2976791.21154493, 3035159.66667327, 3093528.1218016 ,
        3151896.57692993, 3210265.03205826, 3268633.48718659,
        3327001.94231493, 3385370.39744326, 3443738.85257159,
        3502107.30769992, 3560475.76282825, 3618844.21795659,
        3677212.67308492, 3735581.12821325, 3793949.58334158,
        3852318.03846991, 3910686.49359825, 3969054.94872658,
        4027423.40385491, 4085791.85898324, 4144160.31411157,
        4202528.76923991, 4260897.22436824, 4319265.67949657,
        4377634.1346249 , 4436002.58975323, 4494371.04488157,
        4552739.5000099 , 4611107.95513823, 4669476.41026656,
        4727844.86539489, 4786213.32052323, 4844581.77565156,
        4902950.23077989, 4961318.68590822, 5019687.14103656,
        5078055.59616489, 5136424.05129322, 5194792.50642155,
        5253160.96154988, 5311529.41667822, 5369897.87180655,
        5428266.32693488, 5486634.78206321, 5545003.23719154,
        5603371.69231988, 5661740.14744821, 5720108.60257654,
        5778477.05770487],
       [      0.        ,   49510.64983191,   99021.29966381,
         148531.94949572,  198042.59932763,  247553.24915954,
         297063.89899144,  346574.54882335,  396085.19865526,
         445595.84848717,  495106.49831907,  544617.14815098,
         594127.79798289,  643638.44781479,  693149.0976467 ,
         742659.74747861,  792170.39731052,  841681.04714242,
         891191.69697433,  940702.34680624,  990212.99663815,
        1039723.64647005, 1089234.29630196, 1138744.94613387,
        1188255.59596577, 1237766.24579768, 1287276.89562959,
        1336787.5454615 , 1386298.1952934 , 1435808.84512531,
        1485319.49495722, 1534830.14478913, 1584340.79462103,
        1633851.44445294, 1683362.09428485, 1732872.74411676,
        1782383.39394866, 1831894.04378057, 1881404.69361248,
        1930915.34344438, 1980425.99327629, 2029936.6431082 ,
        2079447.29294011, 2128957.94277201, 2178468.59260392,
        2227979.24243583, 2277489.89226774, 2327000.54209964,
        2376511.19193155, 2426021.84176346, 2475532.49159536,
        2525043.14142727, 2574553.79125918, 2624064.44109109,
        2673575.09092299, 2723085.7407549 , 2772596.39058681,
        2822107.04041872, 2871617.69025062, 2921128.34008253,
        2970638.98991444, 3020149.63974635, 3069660.28957825,
        3119170.93941016, 3168681.58924207, 3218192.23907397,
        3267702.88890588, 3317213.53873779, 3366724.1885697 ,
        3416234.8384016 , 3465745.48823351, 3515256.13806542,
        3564766.78789733, 3614277.43772923, 3663788.08756114,
        3713298.73739305, 3762809.38722495, 3812320.03705686,
        3861830.68688877, 3911341.33672068, 3960851.98655258,
        4010362.63638449, 4059873.2862164 , 4109383.93604831,
        4158894.58588021, 4208405.23571212, 4257915.88554403,
        4307426.53537593, 4356937.18520784, 4406447.83503975,
        4455958.48487166, 4505469.13470356, 4554979.78453547,
        4604490.43436738, 4654001.08419929, 4703511.73403119,
        4753022.3838631 , 4802533.03369501, 4852043.68352692,
        4901554.33335882],
       [      0.        ,   44950.39219572,   89900.78439143,
         134851.17658715,  179801.56878286,  224751.96097858,
         269702.35317429,  314652.74537001,  359603.13756572,
         404553.52976144,  449503.92195715,  494454.31415287,
         539404.70634858,  584355.0985443 ,  629305.49074001,
         674255.88293573,  719206.27513144,  764156.66732716,
         809107.05952287,  854057.45171859,  899007.8439143 ,
         943958.23611002,  988908.62830573, 1033859.02050145,
        1078809.41269716, 1123759.80489288, 1168710.19708859,
        1213660.58928431, 1258610.98148002, 1303561.37367574,
        1348511.76587145, 1393462.15806717, 1438412.55026288,
        1483362.9424586 , 1528313.33465431, 1573263.72685003,
        1618214.11904574, 1663164.51124146, 1708114.90343717,
        1753065.29563289, 1798015.6878286 , 1842966.08002432,
        1887916.47222003, 1932866.86441575, 1977817.25661146,
        2022767.64880718, 2067718.04100289, 2112668.43319861,
        2157618.82539433, 2202569.21759004, 2247519.60978576,
        2292470.00198147, 2337420.39417719, 2382370.7863729 ,
        2427321.17856862, 2472271.57076433, 2517221.96296005,
        2562172.35515576, 2607122.74735148, 2652073.13954719,
        2697023.53174291, 2741973.92393862, 2786924.31613434,
        2831874.70833005, 2876825.10052577, 2921775.49272148,
        2966725.8849172 , 3011676.27711291, 3056626.66930863,
        3101577.06150434, 3146527.45370006, 3191477.84589577,
        3236428.23809149, 3281378.6302872 , 3326329.02248292,
        3371279.41467863, 3416229.80687435, 3461180.19907006,
        3506130.59126578, 3551080.98346149, 3596031.37565721,
        3640981.76785292, 3685932.16004864, 3730882.55224435,
        3775832.94444007, 3820783.33663578, 3865733.7288315 ,
        3910684.12102721, 3955634.51322293, 4000584.90541864,
        4045535.29761436, 4090485.68981007, 4135436.08200579,
        4180386.47420151, 4225336.86639722, 4270287.25859294,
        4315237.65078865, 4360188.04298437, 4405138.43518008,
        4450088.8273758 ],
       [      0.        ,   35399.06499842,   70798.12999684,
         106197.19499526,  141596.25999368,  176995.32499209,
         212394.38999051,  247793.45498893,  283192.51998735,
         318591.58498577,  353990.64998419,  389389.71498261,
         424788.77998103,  460187.84497944,  495586.90997786,
         530985.97497628,  566385.0399747 ,  601784.10497312,
         637183.16997154,  672582.23496996,  707981.29996838,
         743380.36496679,  778779.42996521,  814178.49496363,
         849577.55996205,  884976.62496047,  920375.68995889,
         955774.75495731,  991173.81995573, 1026572.88495414,
        1061971.94995256, 1097371.01495098, 1132770.0799494 ,
        1168169.14494782, 1203568.20994624, 1238967.27494466,
        1274366.33994308, 1309765.40494149, 1345164.46993991,
        1380563.53493833, 1415962.59993675, 1451361.66493517,
        1486760.72993359, 1522159.79493201, 1557558.85993043,
        1592957.92492884, 1628356.98992726, 1663756.05492568,
        1699155.1199241 , 1734554.18492252, 1769953.24992094,
        1805352.31491936, 1840751.37991778, 1876150.44491619,
        1911549.50991461, 1946948.57491303, 1982347.63991145,
        2017746.70490987, 2053145.76990829, 2088544.83490671,
        2123943.89990513, 2159342.96490354, 2194742.02990196,
        2230141.09490038, 2265540.1598988 , 2300939.22489722,
        2336338.28989564, 2371737.35489406, 2407136.41989248,
        2442535.48489089, 2477934.54988931, 2513333.61488773,
        2548732.67988615, 2584131.74488457, 2619530.80988299,
        2654929.87488141, 2690328.93987983, 2725728.00487824,
        2761127.06987666, 2796526.13487508, 2831925.1998735 ,
        2867324.26487192, 2902723.32987034, 2938122.39486876,
        2973521.45986718, 3008920.52486559, 3044319.58986401,
        3079718.65486243, 3115117.71986085, 3150516.78485927,
        3185915.84985769, 3221314.91485611, 3256713.97985453,
        3292113.04485294, 3327512.10985136, 3362911.17484978,
        3398310.2398482 , 3433709.30484662, 3469108.36984504,
        3504507.43484346],
       [      0.        ,   52908.8291103 ,  105817.6582206 ,
         158726.48733091,  211635.31644121,  264544.14555151,
         317452.97466181,  370361.80377212,  423270.63288242,
         476179.46199272,  529088.29110302,  581997.12021332,
         634905.94932363,  687814.77843393,  740723.60754423,
         793632.43665453,  846541.26576484,  899450.09487514,
         952358.92398544, 1005267.75309574, 1058176.58220604,
        1111085.41131635, 1163994.24042665, 1216903.06953695,
        1269811.89864725, 1322720.72775755, 1375629.55686786,
        1428538.38597816, 1481447.21508846, 1534356.04419876,
        1587264.87330907, 1640173.70241937, 1693082.53152967,
        1745991.36063997, 1798900.18975027, 1851809.01886058,
        1904717.84797088, 1957626.67708118, 2010535.50619148,
        2063444.33530179, 2116353.16441209, 2169261.99352239,
        2222170.82263269, 2275079.65174299, 2327988.4808533 ,
        2380897.3099636 , 2433806.1390739 , 2486714.9681842 ,
        2539623.79729451, 2592532.62640481, 2645441.45551511,
        2698350.28462541, 2751259.11373571, 2804167.94284602,
        2857076.77195632, 2909985.60106662, 2962894.43017692,
        3015803.25928723, 3068712.08839753, 3121620.91750783,
        3174529.74661813, 3227438.57572843, 3280347.40483874,
        3333256.23394904, 3386165.06305934, 3439073.89216964,
        3491982.72127995, 3544891.55039025, 3597800.37950055,
        3650709.20861085, 3703618.03772115, 3756526.86683146,
        3809435.69594176, 3862344.52505206, 3915253.35416236,
        3968162.18327266, 4021071.01238297, 4073979.84149327,
        4126888.67060357, 4179797.49971387, 4232706.32882418,
        4285615.15793448, 4338523.98704478, 4391432.81615508,
        4444341.64526538, 4497250.47437569, 4550159.30348599,
        4603068.13259629, 4655976.96170659, 4708885.7908169 ,
        4761794.6199272 , 4814703.4490375 , 4867612.2781478 ,
        4920521.1072581 , 4973429.93636841, 5026338.76547871,
        5079247.59458901, 5132156.42369931, 5185065.25280962,
        5237974.08191992],
       [      0.        ,   36496.77018029,   72993.54036059,
         109490.31054088,  145987.08072117,  182483.85090146,
         218980.62108176,  255477.39126205,  291974.16144234,
         328470.93162264,  364967.70180293,  401464.47198322,
         437961.24216352,  474458.01234381,  510954.7825241 ,
         547451.55270439,  583948.32288469,  620445.09306498,
         656941.86324527,  693438.63342557,  729935.40360586,
         766432.17378615,  802928.94396644,  839425.71414674,
         875922.48432703,  912419.25450732,  948916.02468762,
         985412.79486791, 1021909.5650482 , 1058406.3352285 ,
        1094903.10540879, 1131399.87558908, 1167896.64576937,
        1204393.41594967, 1240890.18612996, 1277386.95631025,
        1313883.72649055, 1350380.49667084, 1386877.26685113,
        1423374.03703142, 1459870.80721172, 1496367.57739201,
        1532864.3475723 , 1569361.1177526 , 1605857.88793289,
        1642354.65811318, 1678851.42829348, 1715348.19847377,
        1751844.96865406, 1788341.73883435, 1824838.50901465,
        1861335.27919494, 1897832.04937523, 1934328.81955553,
        1970825.58973582, 2007322.35991611, 2043819.1300964 ,
        2080315.9002767 , 2116812.67045699, 2153309.44063728,
        2189806.21081758, 2226302.98099787, 2262799.75117816,
        2299296.52135846, 2335793.29153875, 2372290.06171904,
        2408786.83189933, 2445283.60207963, 2481780.37225992,
        2518277.14244021, 2554773.91262051, 2591270.6828008 ,
        2627767.45298109, 2664264.22316138, 2700760.99334168,
        2737257.76352197, 2773754.53370226, 2810251.30388256,
        2846748.07406285, 2883244.84424314, 2919741.61442344,
        2956238.38460373, 2992735.15478402, 3029231.92496431,
        3065728.69514461, 3102225.4653249 , 3138722.23550519,
        3175219.00568549, 3211715.77586578, 3248212.54604607,
        3284709.31622636, 3321206.08640666, 3357702.85658695,
        3394199.62676724, 3430696.39694754, 3467193.16712783,
        3503689.93730812, 3540186.70748842, 3576683.47766871,
        3613180.247849  ],
       [      0.        ,   72281.10511212,  144562.21022425,
         216843.31533637,  289124.42044849,  361405.52556062,
         433686.63067274,  505967.73578486,  578248.84089699,
         650529.94600911,  722811.05112123,  795092.15623336,
         867373.26134548,  939654.3664576 , 1011935.47156973,
        1084216.57668185, 1156497.68179398, 1228778.7869061 ,
        1301059.89201822, 1373340.99713035, 1445622.10224247,
        1517903.20735459, 1590184.31246672, 1662465.41757884,
        1734746.52269096, 1807027.62780309, 1879308.73291521,
        1951589.83802733, 2023870.94313946, 2096152.04825158,
        2168433.1533637 , 2240714.25847583, 2312995.36358795,
        2385276.46870007, 2457557.5738122 , 2529838.67892432,
        2602119.78403644, 2674400.88914857, 2746681.99426069,
        2818963.09937281, 2891244.20448494, 2963525.30959706,
        3035806.41470918, 3108087.51982131, 3180368.62493343,
        3252649.73004556, 3324930.83515768, 3397211.9402698 ,
        3469493.04538193, 3541774.15049405, 3614055.25560617,
        3686336.3607183 , 3758617.46583042, 3830898.57094254,
        3903179.67605467, 3975460.78116679, 4047741.88627891,
        4120022.99139104, 4192304.09650316, 4264585.20161528,
        4336866.30672741, 4409147.41183953, 4481428.51695165,
        4553709.62206378, 4625990.7271759 , 4698271.83228802,
        4770552.93740015, 4842834.04251227, 4915115.14762439,
        4987396.25273652, 5059677.35784864, 5131958.46296076,
        5204239.56807289, 5276520.67318501, 5348801.77829714,
        5421082.88340926, 5493363.98852138, 5565645.09363351,
        5637926.19874563, 5710207.30385775, 5782488.40896988,
        5854769.514082  , 5927050.61919412, 5999331.72430625,
        6071612.82941837, 6143893.93453049, 6216175.03964262,
        6288456.14475474, 6360737.24986686, 6433018.35497899,
        6505299.46009111, 6577580.56520323, 6649861.67031536,
        6722142.77542748, 6794423.8805396 , 6866704.98565173,
        6938986.09076385, 7011267.19587597, 7083548.3009881 ,
        7155829.40610022],
       [      0.        ,   77945.14678695,  155890.2935739 ,
         233835.44036085,  311780.5871478 ,  389725.73393475,
         467670.8807217 ,  545616.02750865,  623561.1742956 ,
         701506.32108255,  779451.4678695 ,  857396.61465645,
         935341.7614434 , 1013286.90823035, 1091232.0550173 ,
        1169177.20180425, 1247122.3485912 , 1325067.49537815,
        1403012.6421651 , 1480957.78895205, 1558902.935739  ,
        1636848.08252595, 1714793.2293129 , 1792738.37609984,
        1870683.52288679, 1948628.66967374, 2026573.81646069,
        2104518.96324764, 2182464.11003459, 2260409.25682154,
        2338354.40360849, 2416299.55039544, 2494244.69718239,
        2572189.84396934, 2650134.99075629, 2728080.13754324,
        2806025.28433019, 2883970.43111714, 2961915.57790409,
        3039860.72469104, 3117805.87147799, 3195751.01826494,
        3273696.16505189, 3351641.31183884, 3429586.45862579,
        3507531.60541274, 3585476.75219969, 3663421.89898664,
        3741367.04577359, 3819312.19256054, 3897257.33934749,
        3975202.48613444, 4053147.63292139, 4131092.77970834,
        4209037.92649529, 4286983.07328224, 4364928.22006919,
        4442873.36685614, 4520818.51364309, 4598763.66043004,
        4676708.80721699, 4754653.95400394, 4832599.10079089,
        4910544.24757784, 4988489.39436479, 5066434.54115174,
        5144379.68793869, 5222324.83472564, 5300269.98151258,
        5378215.12829954, 5456160.27508648, 5534105.42187343,
        5612050.56866038, 5689995.71544733, 5767940.86223428,
        5845886.00902123, 5923831.15580818, 6001776.30259513,
        6079721.44938208, 6157666.59616903, 6235611.74295598,
        6313556.88974293, 6391502.03652988, 6469447.18331683,
        6547392.33010378, 6625337.47689073, 6703282.62367768,
        6781227.77046463, 6859172.91725158, 6937118.06403853,
        7015063.21082548, 7093008.35761243, 7170953.50439938,
        7248898.65118633, 7326843.79797328, 7404788.94476023,
        7482734.09154718, 7560679.23833413, 7638624.38512108,
        7716569.53190803],
       [      0.        ,   32230.74592769,   64461.49185538,
          96692.23778306,  128922.98371075,  161153.72963844,
         193384.47556613,  225615.22149381,  257845.9674215 ,
         290076.71334919,  322307.45927688,  354538.20520456,
         386768.95113225,  418999.69705994,  451230.44298763,
         483461.18891531,  515691.934843  ,  547922.68077069,
         580153.42669838,  612384.17262606,  644614.91855375,
         676845.66448144,  709076.41040913,  741307.15633681,
         773537.9022645 ,  805768.64819219,  837999.39411988,
         870230.14004756,  902460.88597525,  934691.63190294,
         966922.37783063,  999153.12375831, 1031383.869686  ,
        1063614.61561369, 1095845.36154138, 1128076.10746906,
        1160306.85339675, 1192537.59932444, 1224768.34525213,
        1256999.09117981, 1289229.8371075 , 1321460.58303519,
        1353691.32896288, 1385922.07489056, 1418152.82081825,
        1450383.56674594, 1482614.31267363, 1514845.05860131,
        1547075.804529  , 1579306.55045669, 1611537.29638438,
        1643768.04231206, 1675998.78823975, 1708229.53416744,
        1740460.28009513, 1772691.02602281, 1804921.7719505 ,
        1837152.51787819, 1869383.26380588, 1901614.00973356,
        1933844.75566125, 1966075.50158894, 1998306.24751663,
        2030536.99344431, 2062767.739372  , 2094998.48529969,
        2127229.23122738, 2159459.97715506, 2191690.72308275,
        2223921.46901044, 2256152.21493813, 2288382.96086582,
        2320613.7067935 , 2352844.45272119, 2385075.19864888,
        2417305.94457657, 2449536.69050425, 2481767.43643194,
        2513998.18235963, 2546228.92828732, 2578459.674215  ,
        2610690.42014269, 2642921.16607038, 2675151.91199807,
        2707382.65792575, 2739613.40385344, 2771844.14978113,
        2804074.89570882, 2836305.6416365 , 2868536.38756419,
        2900767.13349188, 2932997.87941957, 2965228.62534725,
        2997459.37127494, 3029690.11720263, 3061920.86313032,
        3094151.609058  , 3126382.35498569, 3158613.10091338,
        3190843.84684107]])
deal_volume = np.array([4480745.33063313, 5778477.05770487, 4901554.33335882,
       4450088.8273758 , 3504507.43484346, 5237974.08191992,
       3613180.247849  , 7155829.40610022, 7716569.53190803,
       3190843.84684107])
X_value = np.array([[1],
       [0],
       [0],
       [1],
       [0],
       [0],
       [0],
       [1],
       [1],
       [1]])
y_value = np.array([[9.99997886e-01, 9.99997161e-01, 9.99996189e-01, 9.99994883e-01,
        9.99993129e-01, 9.99990774e-01, 9.99987613e-01, 9.99983369e-01,
        9.99977670e-01, 9.99970018e-01, 9.99959744e-01, 9.99945951e-01,
        9.99927431e-01, 9.99902566e-01, 9.99869182e-01, 9.99824362e-01,
        9.99764190e-01, 9.99683410e-01, 9.99574969e-01, 9.99429406e-01,
        9.99234029e-01, 9.98971822e-01, 9.98619980e-01, 9.98147962e-01,
        9.97514897e-01, 9.96666160e-01, 9.95528853e-01, 9.94005899e-01,
        9.91968384e-01, 9.89245770e-01, 9.85613609e-01, 9.80778551e-01,
        9.74360770e-01, 9.65874736e-01, 9.54710559e-01, 9.40120421e-01,
        9.21217886e-01, 8.97002019e-01, 8.66422041e-01, 8.28498783e-01,
        7.82511586e-01, 7.28238011e-01, 6.66198113e-01, 5.97817022e-01,
        5.25407026e-01, 4.51912554e-01, 3.80457221e-01, 3.13830884e-01,
        2.54087098e-01, 2.02362500e-01, 1.58924064e-01, 1.23367808e-01,
        9.48693027e-02, 7.24102947e-02, 5.49453924e-02, 4.15044311e-02,
        3.12427534e-02, 2.34561150e-02, 1.75749351e-02, 1.31484962e-02,
        9.82575393e-03, 7.33645589e-03, 5.47432050e-03, 4.08288731e-03,
        3.04403859e-03, 2.26891206e-03, 1.69082711e-03, 1.25984359e-03,
        9.38612373e-04, 6.99230402e-04, 5.20868138e-04, 3.87985512e-04,
        2.88993780e-04, 2.15253642e-04, 1.60326144e-04, 1.19413135e-04,
        8.89396295e-05, 6.62422620e-05, 4.93369703e-05, 3.67458185e-05,
        2.73679318e-05, 2.03833233e-05, 1.51812395e-05, 1.13067788e-05,
        8.42112530e-06, 6.27192770e-06, 4.67123503e-06, 3.47906239e-06,
        2.59115017e-06, 1.92984687e-06, 1.43731860e-06, 1.07049140e-06,
        7.97284431e-07, 5.93804319e-07, 4.42255659e-07, 3.29384704e-07,
        2.45320276e-07, 1.82710477e-07, 1.36079734e-07, 1.01349929e-07],
       [9.99996876e-01, 9.99995632e-01, 9.99993894e-01, 9.99991465e-01,
        9.99988068e-01, 9.99983320e-01, 9.99976683e-01, 9.99967405e-01,
        9.99954434e-01, 9.99936304e-01, 9.99910959e-01, 9.99875531e-01,
        9.99826009e-01, 9.99756788e-01, 9.99660039e-01, 9.99524822e-01,
        9.99335858e-01, 9.99071818e-01, 9.98702942e-01, 9.98187734e-01,
        9.97468398e-01, 9.96464549e-01, 9.95064616e-01, 9.93114182e-01,
        9.90400386e-01, 9.86631441e-01, 9.81410530e-01, 9.74203972e-01,
        9.64305267e-01, 9.50799965e-01, 9.32542311e-01, 9.08163829e-01,
        8.76145334e-01, 8.34991940e-01, 7.83542715e-01, 7.21404067e-01,
        6.49408801e-01, 5.69900729e-01, 4.86615883e-01, 4.04067692e-01,
        3.26613913e-01, 2.57589428e-01, 1.98844672e-01, 1.50776183e-01,
        1.12693205e-01, 8.32858468e-02, 6.10245852e-02, 4.44251274e-02,
        3.21861577e-02, 2.32369892e-02, 1.67330625e-02, 1.20271397e-02,
        8.63306710e-03, 6.19080386e-03, 4.43635618e-03, 3.17752186e-03,
        2.27507068e-03, 1.62850684e-03, 1.16547860e-03, 8.33991755e-04,
        5.96730474e-04, 4.26938520e-04, 3.05443909e-04, 2.18515666e-04,
        1.56323020e-04, 1.11829301e-04, 7.99986724e-05, 5.72276636e-05,
        4.09379825e-05, 2.92849740e-05, 2.09489282e-05, 1.49857245e-05,
        1.07199545e-05, 7.66845039e-06, 5.48557182e-06, 3.92406262e-06,
        2.80704749e-06, 2.00799882e-06, 1.43640546e-06, 1.02752068e-06,
        7.35028268e-07, 5.25796242e-07, 3.76123864e-07, 2.69056991e-07,
        1.92467613e-07, 1.37680054e-07, 9.84882433e-08, 7.04527174e-08,
        5.03977448e-08, 3.60515926e-08, 2.57891961e-08, 1.84480791e-08,
        1.31966744e-08, 9.44012724e-09, 6.75291362e-09, 4.83063853e-09,
        3.45555561e-09, 2.47190191e-09, 1.76825371e-09, 1.26490504e-09],
       [9.99993496e-01, 9.99991153e-01, 9.99987965e-01, 9.99983628e-01,
        9.99977728e-01, 9.99969703e-01, 9.99958786e-01, 9.99943936e-01,
        9.99923734e-01, 9.99896255e-01, 9.99858876e-01, 9.99808032e-01,
        9.99738875e-01, 9.99644812e-01, 9.99516883e-01, 9.99342907e-01,
        9.99106337e-01, 9.98784698e-01, 9.98347490e-01, 9.97753349e-01,
        9.96946245e-01, 9.95850397e-01, 9.94363523e-01, 9.92347972e-01,
        9.89619202e-01, 9.85931128e-01, 9.80957975e-01, 9.74272751e-01,
        9.65323464e-01, 9.53410012e-01, 9.37667868e-01, 9.17069331e-01,
        8.90458938e-01, 8.56644678e-01, 8.14565978e-01, 7.63545275e-01,
        7.03594773e-01, 6.35696616e-01, 5.61928831e-01, 4.85317524e-01,
        4.09390059e-01, 3.37550762e-01, 2.72501336e-01, 2.15902025e-01,
        1.68338332e-01, 1.29522140e-01, 9.85952663e-02, 7.44216613e-02,
        5.58079981e-02, 4.16403706e-02, 3.09514887e-02, 2.29407118e-02,
        1.69669600e-02, 1.25288237e-02, 9.24068111e-03, 6.80954825e-03,
        5.01478527e-03, 3.69130206e-03, 2.71615404e-03, 1.99809866e-03,
        1.46959208e-03, 1.08072662e-03, 7.94676086e-04, 5.84294130e-04,
        4.29584591e-04, 3.15826136e-04, 2.32185102e-04, 1.70691152e-04,
        1.25481758e-04, 9.22454556e-05, 6.78118419e-05, 4.98497851e-05,
        3.66453584e-05, 2.69384832e-05, 1.98027811e-05, 1.45572189e-05,
        1.07011397e-05, 7.86649399e-06, 5.78271872e-06, 4.25091756e-06,
        3.12487838e-06, 2.29711865e-06, 1.68862665e-06, 1.24132009e-06,
        9.12502103e-07, 6.70785904e-07, 4.93098808e-07, 3.62479924e-07,
        2.66461181e-07, 1.95877218e-07, 1.43990519e-07, 1.05848294e-07,
        7.78097149e-08, 5.71983869e-08, 4.20468762e-08, 3.09089100e-08,
        2.27213244e-08, 1.67025812e-08, 1.22781672e-08, 9.02575401e-09],
       [9.99999167e-01, 9.99998882e-01, 9.99998500e-01, 9.99997988e-01,
        9.99997302e-01, 9.99996381e-01, 9.99995145e-01, 9.99993487e-01,
        9.99991264e-01, 9.99988282e-01, 9.99984281e-01, 9.99978915e-01,
        9.99971717e-01, 9.99962062e-01, 9.99949111e-01, 9.99931739e-01,
        9.99908437e-01, 9.99877182e-01, 9.99835260e-01, 9.99779031e-01,
        9.99703616e-01, 9.99602472e-01, 9.99466831e-01, 9.99284941e-01,
        9.99041058e-01, 9.98714102e-01, 9.98275862e-01, 9.97688613e-01,
        9.96901965e-01, 9.95848706e-01, 9.94439362e-01, 9.92555129e-01,
        9.90038816e-01, 9.86683417e-01, 9.82218061e-01, 9.76291348e-01,
        9.68452709e-01, 9.58133570e-01, 9.44632011e-01, 9.27107568e-01,
        9.04596673e-01, 8.76063221e-01, 8.40500615e-01, 7.97097035e-01,
        7.45459246e-01, 6.85859068e-01, 6.19428074e-01, 5.48202914e-01,
        4.74946821e-01, 4.02753368e-01, 3.34541616e-01, 2.72609348e-01,
        2.18379846e-01, 1.72381539e-01, 1.34405985e-01, 1.03747689e-01,
        7.94408056e-02, 6.04446763e-02, 4.57651359e-02, 3.45196738e-02,
        2.59622788e-02, 1.94834447e-02, 1.45971610e-02, 1.09226651e-02,
        8.16547341e-03, 6.09998298e-03, 4.55456770e-03, 3.39934037e-03,
        2.53638003e-03, 1.89207585e-03, 1.41120954e-03, 1.05242533e-03,
        7.84786297e-04, 5.85169850e-04, 4.36305215e-04, 3.25298762e-04,
        2.42528145e-04, 1.80814282e-04, 1.34802050e-04, 1.00497481e-04,
        7.49221206e-05, 5.58550083e-05, 4.16401296e-05, 3.10427690e-05,
        2.31423610e-05, 1.72525780e-05, 1.28617388e-05, 9.58837222e-06,
        7.14808524e-06, 5.32885977e-06, 3.97263495e-06, 2.96157596e-06,
        2.20783687e-06, 1.64592865e-06, 1.22702939e-06, 9.14742545e-07,
        6.81934650e-07, 5.08377840e-07, 3.78992352e-07, 2.82536309e-07],
       [9.99094974e-01, 9.98821485e-01, 9.98465478e-01, 9.98002143e-01,
        9.97399273e-01, 9.96615099e-01, 9.95595523e-01, 9.94270603e-01,
        9.92550114e-01, 9.90318008e-01, 9.87425599e-01, 9.83683346e-01,
        9.78851218e-01, 9.72627891e-01, 9.64639417e-01, 9.54428765e-01,
        9.41448666e-01, 9.25061718e-01, 9.04553471e-01, 8.79165878e-01,
        8.48159042e-01, 8.10906962e-01, 7.67025995e-01, 7.16521802e-01,
        6.59923449e-01, 5.98359118e-01, 5.33527710e-01, 4.67543711e-01,
        4.02675781e-01, 3.41043233e-01, 2.84353320e-01, 2.33744077e-01,
        1.89753949e-01, 1.52396136e-01, 1.21292142e-01, 9.58188058e-02,
        7.52372222e-02, 5.87890588e-02, 4.57588206e-02, 3.55077015e-02,
        2.74869383e-02, 2.12380735e-02, 1.63858861e-02, 1.26279582e-02,
        9.72335128e-03, 7.48178205e-03, 5.75396956e-03, 4.42339279e-03,
        3.39945324e-03, 2.61191629e-03, 2.00645737e-03, 1.54113085e-03,
        1.18359230e-03, 9.08926300e-04, 6.97955117e-04, 5.35926257e-04,
        4.11496578e-04, 3.15947460e-04, 2.42579373e-04, 1.86245363e-04,
        1.42991882e-04, 1.09782454e-04, 8.42851645e-05, 6.47093101e-05,
        4.96798671e-05, 3.81410430e-05, 2.92821891e-05, 2.24808963e-05,
        1.72592937e-05, 1.32504884e-05, 1.01727964e-05, 7.80995456e-06,
        5.99592819e-06, 4.60324570e-06, 3.53404234e-06, 2.71318391e-06,
        2.08298736e-06, 1.59916757e-06, 1.22772546e-06, 9.42558936e-07,
        7.23628625e-07, 5.55549728e-07, 4.26510889e-07, 3.27444195e-07,
        2.51387950e-07, 1.92997467e-07, 1.48169480e-07, 1.13753797e-07,
        8.73319264e-08, 6.70471277e-08, 5.14739282e-08, 3.95179535e-08,
        3.03390221e-08, 2.32921034e-08, 1.78819897e-08, 1.37284963e-08,
        1.05397449e-08, 8.09165259e-09, 6.21218467e-09, 4.76926535e-09],
       [9.99808712e-01, 9.99737066e-01, 9.99638594e-01, 9.99503262e-01,
        9.99317287e-01, 9.99061751e-01, 9.98710693e-01, 9.98228515e-01,
        9.97566449e-01, 9.96657775e-01, 9.95411368e-01, 9.93703079e-01,
        9.91364332e-01, 9.88167293e-01, 9.83806000e-01, 9.77873220e-01,
        9.69833572e-01, 9.58995253e-01, 9.44485796e-01, 9.25242421e-01,
        9.00034560e-01, 8.67543582e-01, 8.26527704e-01, 7.76089074e-01,
        7.16023688e-01, 6.47170734e-01, 5.71611561e-01, 4.92558741e-01,
        4.13876284e-01, 3.39357137e-01, 2.72028137e-01, 2.13735590e-01,
        1.65101462e-01, 1.25763635e-01, 9.47350940e-02, 7.07424803e-02,
        5.24740179e-02, 3.87265777e-02, 2.84725429e-02, 2.08746153e-02,
        1.52723302e-02, 1.11564428e-02, 8.14061576e-03, 5.93513860e-03,
        4.32456995e-03, 3.14966300e-03, 2.29322171e-03, 1.66926987e-03,
        1.21487941e-03, 8.84068639e-04, 6.43279381e-04, 4.68041941e-04,
        3.40525129e-04, 2.47741331e-04, 1.80234026e-04, 1.31119448e-04,
        9.53875345e-05, 6.93923998e-05, 5.04811353e-05, 3.67235014e-05,
        2.67151380e-05, 1.94343302e-05, 1.41377686e-05, 1.02846978e-05,
        7.48172510e-06, 5.44266527e-06, 3.95932601e-06, 2.88025357e-06,
        2.09527031e-06, 1.52422577e-06, 1.10881342e-06, 8.06617420e-07,
        5.86781868e-07, 4.26860283e-07, 3.10523728e-07, 2.25893545e-07,
        1.64328481e-07, 1.19542368e-07, 8.69622686e-08, 6.32615552e-08,
        4.60202384e-08, 3.34778734e-08, 2.43538070e-08, 1.77164154e-08,
        1.28879799e-08, 9.37548720e-09, 6.82028997e-09, 4.96148672e-09,
        3.60928209e-09, 2.62560760e-09, 1.91002395e-09, 1.38946563e-09,
        1.01078037e-09, 7.35302078e-10, 5.34902696e-10, 3.89120203e-10,
        2.83069301e-10, 2.05921534e-10, 1.49799636e-10, 1.08973212e-10],
       [9.98841512e-01, 9.98486450e-01, 9.98022781e-01, 9.97417437e-01,
        9.96627387e-01, 9.95596714e-01, 9.94252883e-01, 9.92502020e-01,
        9.90223000e-01, 9.87260161e-01, 9.83414496e-01, 9.78433333e-01,
        9.71998762e-01, 9.63715585e-01, 9.53100350e-01, 9.39574283e-01,
        9.22464584e-01, 9.01020590e-01, 8.74453000e-01, 8.42004718e-01,
        8.03058769e-01, 7.57280014e-01, 7.04772064e-01, 6.46211782e-01,
        5.82910029e-01, 5.16751974e-01, 4.50001428e-01, 3.85003812e-01,
        3.23866150e-01, 2.68203067e-01, 2.19008014e-01, 1.76658223e-01,
        1.41018974e-01, 1.11595117e-01, 8.76839021e-02, 6.85010421e-02,
        5.32698284e-02, 4.12752119e-02, 3.18904135e-02, 2.45847307e-02,
        1.89199774e-02, 1.45410244e-02, 1.11640298e-02, 8.56448938e-03,
        6.56623162e-03, 5.03183902e-03, 3.85461102e-03, 2.95198521e-03,
        2.26024572e-03, 1.73032043e-03, 1.32447369e-03, 1.01372166e-03,
        7.75822576e-04, 5.93720205e-04, 4.54341771e-04, 3.47671656e-04,
        2.66038827e-04, 2.03569372e-04, 1.55766290e-04, 1.19187206e-04,
        9.11973189e-05, 6.97801097e-05, 5.33923505e-05, 4.08530759e-05,
        3.12585770e-05, 2.39173284e-05, 1.83001808e-05, 1.40022402e-05,
        1.07136937e-05, 8.19748430e-06, 6.27222609e-06, 4.79913116e-06,
        3.67200601e-06, 2.80959702e-06, 2.14973335e-06, 1.64484542e-06,
        1.25853568e-06, 9.62954873e-07, 7.36794386e-07, 5.63750134e-07,
        4.31347208e-07, 3.30040556e-07, 2.52526883e-07, 1.93218150e-07,
        1.47838727e-07, 1.13117163e-07, 8.65503424e-08, 6.62230340e-08,
        5.06698193e-08, 3.87694496e-08, 2.96640138e-08, 2.26970907e-08,
        1.73664268e-08, 1.32877285e-08, 1.01669579e-08, 7.77913484e-09,
        5.95211858e-09, 4.55419739e-09, 3.48459353e-09, 2.66619802e-09],
       [9.99999964e-01, 9.99999947e-01, 9.99999923e-01, 9.99999888e-01,
        9.99999836e-01, 9.99999761e-01, 9.99999651e-01, 9.99999491e-01,
        9.99999258e-01, 9.99998917e-01, 9.99998420e-01, 9.99997695e-01,
        9.99996636e-01, 9.99995092e-01, 9.99992839e-01, 9.99989552e-01,
        9.99984756e-01, 9.99977758e-01, 9.99967548e-01, 9.99952650e-01,
        9.99930914e-01, 9.99899202e-01, 9.99852935e-01, 9.99785435e-01,
        9.99686964e-01, 9.99543322e-01, 9.99333812e-01, 9.99028278e-01,
        9.98582816e-01, 9.97933565e-01, 9.96987771e-01, 9.95610997e-01,
        9.93608987e-01, 9.90702304e-01, 9.86491610e-01, 9.80411704e-01,
        9.71673904e-01, 9.59200614e-01, 9.41565071e-01, 9.16966447e-01,
        8.83296309e-01, 8.38379381e-01, 7.80471376e-01, 7.09016427e-01,
        6.25464580e-01, 5.33700022e-01, 4.39596749e-01, 3.49644470e-01,
        2.69254956e-01, 2.01618468e-01, 1.47541755e-01, 1.06042662e-01,
        7.51865194e-02, 5.27787686e-02, 3.67835565e-02, 2.55053501e-02,
        1.76218909e-02, 1.21447656e-02, 8.35552663e-03, 5.74168108e-03,
        3.94226980e-03, 2.70524976e-03, 1.85566331e-03, 1.27254990e-03,
        8.72510663e-04, 5.98152584e-04, 4.10030087e-04, 2.81056580e-04,
        1.92643413e-04, 1.32039080e-04, 9.04987401e-05, 6.20264463e-05,
        4.25115927e-05, 2.91363527e-05, 1.99692230e-05, 1.36862950e-05,
        9.38014965e-06, 6.42884640e-06, 4.40611602e-06, 3.01980243e-06,
        2.06966923e-06, 1.41848003e-06, 9.72177178e-07, 6.66296535e-07,
        4.56656503e-07, 3.12976486e-07, 2.14503189e-07, 1.47013014e-07,
        1.00757597e-07, 6.90557454e-08, 4.73284005e-08, 3.24372355e-08,
        2.22313502e-08, 1.52365922e-08, 1.04426289e-08, 7.15701368e-09,
        4.90516757e-09, 3.36183079e-09, 2.30408157e-09, 1.57913715e-09],
       [9.99999972e-01, 9.99999959e-01, 9.99999938e-01, 9.99999909e-01,
        9.99999864e-01, 9.99999799e-01, 9.99999701e-01, 9.99999556e-01,
        9.99999341e-01, 9.99999021e-01, 9.99998547e-01, 9.99997842e-01,
        9.99996796e-01, 9.99995243e-01, 9.99992937e-01, 9.99989514e-01,
        9.99984431e-01, 9.99976884e-01, 9.99965679e-01, 9.99949044e-01,
        9.99924346e-01, 9.99887678e-01, 9.99833242e-01, 9.99752429e-01,
        9.99632469e-01, 9.99454413e-01, 9.99190167e-01, 9.98798090e-01,
        9.98216530e-01, 9.97354319e-01, 9.96076916e-01, 9.94186344e-01,
        9.91392560e-01, 9.87273394e-01, 9.81220307e-01, 9.72368797e-01,
        9.59517397e-01, 9.41051115e-01, 9.14908380e-01, 8.78666662e-01,
        8.29859609e-01, 7.66633959e-01, 6.88725978e-01, 5.98432593e-01,
        5.00926776e-01, 4.03350418e-01, 3.12865719e-01, 2.34695112e-01,
        1.71189785e-01, 1.22126000e-01, 8.56706031e-02, 5.93615282e-02,
        4.07715823e-02, 2.78311047e-02, 1.89168036e-02, 1.28201001e-02,
        8.67093926e-03, 5.85666664e-03, 3.95216339e-03, 2.66531664e-03,
        1.79671865e-03, 1.21084373e-03, 8.15855049e-04, 5.49644508e-04,
        3.70265311e-04, 2.49412791e-04, 1.67999226e-04, 1.13157748e-04,
        7.62172956e-05, 5.13354692e-05, 3.45762595e-05, 2.32882098e-05,
        1.56852918e-05, 1.05644774e-05, 7.11545558e-06, 4.79244255e-06,
        3.22783100e-06, 2.17402453e-06, 1.46425914e-06, 9.86214412e-07,
        6.64239471e-07, 4.47381444e-07, 3.01322266e-07, 2.02947853e-07,
        1.36690296e-07, 9.20642243e-08, 6.20074834e-08, 4.17635405e-08,
        2.81287549e-08, 1.89453969e-08, 1.27601830e-08, 8.59429189e-09,
        5.78846345e-09, 3.89867013e-09, 2.62584862e-09, 1.76857255e-09,
        1.19117638e-09, 8.02286101e-10, 5.40359091e-10, 3.63944916e-10],
       [9.99992361e-01, 9.99990147e-01, 9.99987290e-01, 9.99983606e-01,
        9.99978853e-01, 9.99972723e-01, 9.99964815e-01, 9.99954615e-01,
        9.99941459e-01, 9.99924488e-01, 9.99902599e-01, 9.99874365e-01,
        9.99837948e-01, 9.99790977e-01, 9.99730396e-01, 9.99652263e-01,
        9.99551496e-01, 9.99421545e-01, 9.99253971e-01, 9.99037899e-01,
        9.98759323e-01, 9.98400215e-01, 9.97937380e-01, 9.97340998e-01,
        9.96572771e-01, 9.95583574e-01, 9.94310497e-01, 9.92673144e-01,
        9.90569055e-01, 9.87868109e-01, 9.84405812e-01, 9.79975445e-01,
        9.74319231e-01, 9.67118947e-01, 9.57986924e-01, 9.46459081e-01,
        9.31992695e-01, 9.13972870e-01, 8.91733013e-01, 8.64595458e-01,
        8.31937744e-01, 7.93286628e-01, 7.48434400e-01, 6.97560520e-01,
        6.41328851e-01, 5.80923287e-01, 5.17990491e-01, 4.54481716e-01,
        3.92420200e-01, 3.33650115e-01, 2.79630803e-01, 2.31321309e-01,
        1.89166209e-01, 1.53162584e-01, 1.22972197e-01, 9.80437604e-02,
        7.77208938e-02, 6.13242093e-02, 4.82059081e-02, 3.77808884e-02,
        2.95404132e-02, 2.30542232e-02, 1.79658410e-02, 1.39844581e-02,
        1.08756122e-02, 8.45196107e-03, 6.56484049e-03, 5.09690288e-03,
        3.95589784e-03, 3.06953289e-03, 2.38129352e-03, 1.84708274e-03,
        1.43254278e-03, 1.11093430e-03, 8.61465222e-04, 6.67978935e-04,
        5.17927410e-04, 4.01569183e-04, 3.11343998e-04, 2.41385852e-04,
        1.87144168e-04, 1.45089335e-04, 1.12483954e-04, 8.72052191e-05,
        6.76070387e-05, 5.24130504e-05, 4.06336163e-05, 3.15014370e-05,
        2.44216141e-05, 1.89329231e-05, 1.46777823e-05, 1.13789660e-05,
        8.82154864e-06, 6.83890573e-06, 5.30185943e-06, 4.11026336e-06,
        3.18647837e-06, 2.47031428e-06, 1.91510845e-06, 1.48468556e-06]])

# toy sample
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]
y = [-1, -1, -1, 1, 1, 1]
T = [[-1, -1], [2, 2], [3, 2]]
true_result = [-1, 1, 1]

# also load the iris dataset
# and randomly permute it
iris = datasets.load_iris()
rng = np.random.RandomState(1)
perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]

# also load the diabetes dataset
# and randomly permute it
diabetes = datasets.load_diabetes()
perm = rng.permutation(diabetes.target.size)
diabetes.data = diabetes.data[perm]
diabetes.target = diabetes.target[perm]

digits = datasets.load_digits()
perm = rng.permutation(digits.target.size)
digits.data = digits.data[perm]
digits.target = digits.target[perm]

random_state = check_random_state(0)
X_multilabel, y_multilabel = datasets.make_multilabel_classification(
    random_state=0, n_samples=30, n_features=10
)

# NB: despite their names X_sparse_* are numpy arrays (and not sparse matrices)
X_sparse_pos = random_state.uniform(size=(20, 5))
X_sparse_pos[X_sparse_pos <= 0.8] = 0.0
y_random = random_state.randint(0, 4, size=(20,))
X_sparse_mix = _sparse_random_matrix(20, 10, density=0.25, random_state=0).toarray()


DATASETS = {
    "iris": {"X": iris.data, "y": iris.target},
    "diabetes": {"X": diabetes.data, "y": diabetes.target},
    "digits": {"X": digits.data, "y": digits.target},
    "toy": {"X": X, "y": y},
    "clf_small": {"X": X_small, "y": y_small},
    "reg_small": {"X": X_small, "y": y_small_reg},
    "multilabel": {"X": X_multilabel, "y": y_multilabel},
    "sparse-pos": {"X": X_sparse_pos, "y": y_random},
    "sparse-neg": {"X": -X_sparse_pos, "y": y_random},
    "sparse-mix": {"X": X_sparse_mix, "y": y_random},
    "zeros": {"X": np.zeros((20, 3)), "y": y_random},
}


def assert_tree_equal(d, s, message):
    assert (
        s.node_count == d.node_count
    ), "{0}: inequal number of node ({1} != {2})".format(
        message, s.node_count, d.node_count
    )

    assert_array_equal(
        d.children_right, s.children_right, message + ": inequal children_right"
    )
    assert_array_equal(
        d.children_left, s.children_left, message + ": inequal children_left"
    )

    external = d.children_right == TREE_LEAF
    internal = np.logical_not(external)

    assert_array_equal(
        d.feature[internal], s.feature[internal], message + ": inequal features"
    )
    assert_array_equal(
        d.threshold[internal], s.threshold[internal], message + ": inequal threshold"
    )
    assert_array_equal(
        d.n_node_samples.sum(),
        s.n_node_samples.sum(),
        message + ": inequal sum(n_node_samples)",
    )
    assert_array_equal(
        d.n_node_samples, s.n_node_samples, message + ": inequal n_node_samples"
    )

    assert_almost_equal(d.impurity, s.impurity, err_msg=message + ": inequal impurity")

    assert_array_almost_equal(
        d.value[external], s.value[external], err_msg=message + ": inequal value"
    )


def test_classification_toy():
    # Check classification on a toy dataset.
    for name, Tree in CLF_TREES.items():
        clf = Tree(random_state=0)
        clf.fit(X, y)
        assert_array_equal(clf.predict(T), true_result, "Failed with {0}".format(name))

        clf = Tree(max_features=1, random_state=1)
        clf.fit(X, y)
        assert_array_equal(clf.predict(T), true_result, "Failed with {0}".format(name))


def test_weighted_classification_toy():
    # Check classification on a weighted toy dataset.
    for name, Tree in CLF_TREES.items():
        clf = Tree(random_state=0)

        clf.fit(X, y, sample_weight=np.ones(len(X)))
        assert_array_equal(clf.predict(T), true_result, "Failed with {0}".format(name))

        clf.fit(X, y, sample_weight=np.full(len(X), 0.5))
        assert_array_equal(clf.predict(T), true_result, "Failed with {0}".format(name))


@pytest.mark.parametrize("Tree", REG_TREES.values())
@pytest.mark.parametrize("criterion", REG_CRITERIONS)
def test_regression_toy(Tree, criterion):
    # Check regression on a toy dataset.
    if criterion == "poisson":
        # make target positive while not touching the original y and
        # true_result
        a = np.abs(np.min(y)) + 1
        y_train = np.array(y) + a
        y_test = np.array(true_result) + a
    else:
        y_train = y
        y_test = true_result

    reg = Tree(criterion=criterion, random_state=1)
    reg.fit(X, y_train)
    assert_allclose(reg.predict(T), y_test)

    clf = Tree(criterion=criterion, max_features=1, random_state=1)
    clf.fit(X, y_train)
    assert_allclose(reg.predict(T), y_test)


def test_xor():
    # Check on a XOR problem
    y = np.zeros((10, 10))
    y[:5, :5] = 1
    y[5:, 5:] = 1

    gridx, gridy = np.indices(y.shape)

    X = np.vstack([gridx.ravel(), gridy.ravel()]).T
    y = y.ravel()

    for name, Tree in CLF_TREES.items():
        clf = Tree(random_state=0)
        clf.fit(X, y)
        assert clf.score(X, y) == 1.0, "Failed with {0}".format(name)

        clf = Tree(random_state=0, max_features=1)
        clf.fit(X, y)
        assert clf.score(X, y) == 1.0, "Failed with {0}".format(name)


def test_iris():
    # Check consistency on dataset iris.
    for (name, Tree), criterion in product(CLF_TREES.items(), CLF_CRITERIONS):
        clf = Tree(criterion=criterion, random_state=0)
        clf.fit(iris.data, iris.target)
        score = accuracy_score(clf.predict(iris.data), iris.target)
        assert score > 0.9, "Failed with {0}, criterion = {1} and score = {2}".format(
            name, criterion, score
        )

        clf = Tree(criterion=criterion, max_features=2, random_state=0)
        clf.fit(iris.data, iris.target)
        score = accuracy_score(clf.predict(iris.data), iris.target)
        assert score > 0.5, "Failed with {0}, criterion = {1} and score = {2}".format(
            name, criterion, score
        )


@pytest.mark.parametrize("name, Tree", REG_TREES.items())
@pytest.mark.parametrize("criterion", REG_CRITERIONS)
def test_diabetes_overfit(name, Tree, criterion):
    # check consistency of overfitted trees on the diabetes dataset
    # since the trees will overfit, we expect an MSE of 0
    reg = Tree(criterion=criterion, random_state=0)
    reg.fit(diabetes.data, diabetes.target)
    score = mean_squared_error(diabetes.target, reg.predict(diabetes.data))
    assert score == pytest.approx(
        0
    ), f"Failed with {name}, criterion = {criterion} and score = {score}"


@skip_if_32bit
@pytest.mark.parametrize("name, Tree", REG_TREES.items())
@pytest.mark.parametrize(
    "criterion, max_depth, metric, max_loss",
    [
        ("squared_error", 15, mean_squared_error, 60),
        ("absolute_error", 20, mean_squared_error, 60),
        ("friedman_mse", 15, mean_squared_error, 60),
        ("poisson", 15, mean_poisson_deviance, 30),
    ],
)
def test_diabetes_underfit(name, Tree, criterion, max_depth, metric, max_loss):
    # check consistency of trees when the depth and the number of features are
    # limited

    reg = Tree(criterion=criterion, max_depth=max_depth, max_features=6, random_state=0)
    reg.fit(diabetes.data, diabetes.target)
    loss = metric(diabetes.target, reg.predict(diabetes.data))
    assert 0 < loss < max_loss


def test_probability():
    # Predict probabilities using DecisionTreeClassifier.

    for name, Tree in CLF_TREES.items():
        clf = Tree(max_depth=1, max_features=1, random_state=42)
        clf.fit(iris.data, iris.target)

        prob_predict = clf.predict_proba(iris.data)
        assert_array_almost_equal(
            np.sum(prob_predict, 1),
            np.ones(iris.data.shape[0]),
            err_msg="Failed with {0}".format(name),
        )
        assert_array_equal(
            np.argmax(prob_predict, 1),
            clf.predict(iris.data),
            err_msg="Failed with {0}".format(name),
        )
        assert_almost_equal(
            clf.predict_proba(iris.data),
            np.exp(clf.predict_log_proba(iris.data)),
            8,
            err_msg="Failed with {0}".format(name),
        )


def test_arrayrepr():
    # Check the array representation.
    # Check resize
    X = np.arange(10000)[:, np.newaxis]
    y = np.arange(10000)

    for name, Tree in REG_TREES.items():
        reg = Tree(max_depth=None, random_state=0)
        reg.fit(X, y)


def test_pure_set():
    # Check when y is pure.
    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]
    y = [1, 1, 1, 1, 1, 1]

    for name, TreeClassifier in CLF_TREES.items():
        clf = TreeClassifier(random_state=0)
        clf.fit(X, y)
        assert_array_equal(clf.predict(X), y, err_msg="Failed with {0}".format(name))

    for name, TreeRegressor in REG_TREES.items():
        reg = TreeRegressor(random_state=0)
        reg.fit(X, y)
        assert_almost_equal(reg.predict(X), y, err_msg="Failed with {0}".format(name))


def test_numerical_stability():
    # Check numerical stability.
    X = np.array(
        [
            [152.08097839, 140.40744019, 129.75102234, 159.90493774],
            [142.50700378, 135.81935120, 117.82884979, 162.75781250],
            [127.28772736, 140.40744019, 129.75102234, 159.90493774],
            [132.37025452, 143.71923828, 138.35694885, 157.84558105],
            [103.10237122, 143.71928406, 138.35696411, 157.84559631],
            [127.71276855, 143.71923828, 138.35694885, 157.84558105],
            [120.91514587, 140.40744019, 129.75102234, 159.90493774],
        ]
    )

    y = np.array([1.0, 0.70209277, 0.53896582, 0.0, 0.90914464, 0.48026916, 0.49622521])

    with np.errstate(all="raise"):
        for name, Tree in REG_TREES.items():
            reg = Tree(random_state=0)
            reg.fit(X, y)
            reg.fit(X, -y)
            reg.fit(-X, y)
            reg.fit(-X, -y)


def test_importances():
    # Check variable importances.
    X, y = datasets.make_classification(
        n_samples=5000,
        n_features=10,
        n_informative=3,
        n_redundant=0,
        n_repeated=0,
        shuffle=False,
        random_state=0,
    )

    for name, Tree in CLF_TREES.items():
        clf = Tree(random_state=0)

        clf.fit(X, y)
        importances = clf.feature_importances_
        n_important = np.sum(importances > 0.1)

        assert importances.shape[0] == 10, "Failed with {0}".format(name)
        assert n_important == 3, "Failed with {0}".format(name)

    # Check on iris that importances are the same for all builders
    clf = DecisionTreeClassifier(random_state=0)
    clf.fit(iris.data, iris.target)
    clf2 = DecisionTreeClassifier(random_state=0, max_leaf_nodes=len(iris.data))
    clf2.fit(iris.data, iris.target)

    assert_array_equal(clf.feature_importances_, clf2.feature_importances_)


def test_importances_raises():
    # Check if variable importance before fit raises ValueError.
    clf = DecisionTreeClassifier()
    with pytest.raises(ValueError):
        getattr(clf, "feature_importances_")


def test_importances_gini_equal_squared_error():
    # Check that gini is equivalent to squared_error for binary output variable

    X, y = datasets.make_classification(
        n_samples=2000,
        n_features=10,
        n_informative=3,
        n_redundant=0,
        n_repeated=0,
        shuffle=False,
        random_state=0,
    )

    # The gini index and the mean square error (variance) might differ due
    # to numerical instability. Since those instabilities mainly occurs at
    # high tree depth, we restrict this maximal depth.
    clf = DecisionTreeClassifier(criterion="gini", max_depth=5, random_state=0).fit(
        X, y
    )
    reg = DecisionTreeRegressor(
        criterion="squared_error", max_depth=5, random_state=0
    ).fit(X, y)

    assert_almost_equal(clf.feature_importances_, reg.feature_importances_)
    assert_array_equal(clf.tree_.feature, reg.tree_.feature)
    assert_array_equal(clf.tree_.children_left, reg.tree_.children_left)
    assert_array_equal(clf.tree_.children_right, reg.tree_.children_right)
    assert_array_equal(clf.tree_.n_node_samples, reg.tree_.n_node_samples)


def test_max_features():
    # Check max_features.
    for name, TreeEstimator in ALL_TREES.items():
        est = TreeEstimator(max_features="sqrt")
        est.fit(iris.data, iris.target)
        assert est.max_features_ == int(np.sqrt(iris.data.shape[1]))

        est = TreeEstimator(max_features="log2")
        est.fit(iris.data, iris.target)
        assert est.max_features_ == int(np.log2(iris.data.shape[1]))

        est = TreeEstimator(max_features=1)
        est.fit(iris.data, iris.target)
        assert est.max_features_ == 1

        est = TreeEstimator(max_features=3)
        est.fit(iris.data, iris.target)
        assert est.max_features_ == 3

        est = TreeEstimator(max_features=0.01)
        est.fit(iris.data, iris.target)
        assert est.max_features_ == 1

        est = TreeEstimator(max_features=0.5)
        est.fit(iris.data, iris.target)
        assert est.max_features_ == int(0.5 * iris.data.shape[1])

        est = TreeEstimator(max_features=1.0)
        est.fit(iris.data, iris.target)
        assert est.max_features_ == iris.data.shape[1]

        est = TreeEstimator(max_features=None)
        est.fit(iris.data, iris.target)
        assert est.max_features_ == iris.data.shape[1]


def test_error():
    # Test that it gives proper exception on deficient input.
    for name, TreeEstimator in CLF_TREES.items():
        # predict before fit
        est = TreeEstimator()
        with pytest.raises(NotFittedError):
            est.predict_proba(X)

        est.fit(X, y)
        X2 = [[-2, -1, 1]]  # wrong feature shape for sample
        with pytest.raises(ValueError):
            est.predict_proba(X2)

        # Wrong dimensions
        est = TreeEstimator()
        y2 = y[:-1]
        with pytest.raises(ValueError):
            est.fit(X, y2)

        # Test with arrays that are non-contiguous.
        Xf = np.asfortranarray(X)
        est = TreeEstimator()
        est.fit(Xf, y)
        assert_almost_equal(est.predict(T), true_result)

        # predict before fitting
        est = TreeEstimator()
        with pytest.raises(NotFittedError):
            est.predict(T)

        # predict on vector with different dims
        est.fit(X, y)
        t = np.asarray(T)
        with pytest.raises(ValueError):
            est.predict(t[:, 1:])

        # wrong sample shape
        Xt = np.array(X).T

        est = TreeEstimator()
        est.fit(np.dot(X, Xt), y)
        with pytest.raises(ValueError):
            est.predict(X)
        with pytest.raises(ValueError):
            est.apply(X)

        clf = TreeEstimator()
        clf.fit(X, y)
        with pytest.raises(ValueError):
            clf.predict(Xt)
        with pytest.raises(ValueError):
            clf.apply(Xt)

        # apply before fitting
        est = TreeEstimator()
        with pytest.raises(NotFittedError):
            est.apply(T)

    # non positive target for Poisson splitting Criterion
    est = DecisionTreeRegressor(criterion="poisson")
    with pytest.raises(ValueError, match="y is not positive.*Poisson"):
        est.fit([[0, 1, 2]], [0, 0, 0])
    with pytest.raises(ValueError, match="Some.*y are negative.*Poisson"):
        est.fit([[0, 1, 2]], [5, -0.1, 2])


def test_min_samples_split():
    """Test min_samples_split parameter"""
    X = np.asfortranarray(iris.data, dtype=tree._tree.DTYPE)
    y = iris.target

    # test both DepthFirstTreeBuilder and BestFirstTreeBuilder
    # by setting max_leaf_nodes
    for max_leaf_nodes, name in product((None, 1000), ALL_TREES.keys()):
        TreeEstimator = ALL_TREES[name]

        # test for integer parameter
        est = TreeEstimator(
            min_samples_split=10, max_leaf_nodes=max_leaf_nodes, random_state=0
        )
        est.fit(X, y)
        # count samples on nodes, -1 means it is a leaf
        node_samples = est.tree_.n_node_samples[est.tree_.children_left != -1]

        assert np.min(node_samples) > 9, "Failed with {0}".format(name)

        # test for float parameter
        est = TreeEstimator(
            min_samples_split=0.2, max_leaf_nodes=max_leaf_nodes, random_state=0
        )
        est.fit(X, y)
        # count samples on nodes, -1 means it is a leaf
        node_samples = est.tree_.n_node_samples[est.tree_.children_left != -1]

        assert np.min(node_samples) > 9, "Failed with {0}".format(name)


def test_min_samples_leaf():
    # Test if leaves contain more than leaf_count training examples
    X = np.asfortranarray(iris.data, dtype=tree._tree.DTYPE)
    y = iris.target

    # test both DepthFirstTreeBuilder and BestFirstTreeBuilder
    # by setting max_leaf_nodes
    for max_leaf_nodes, name in product((None, 1000), ALL_TREES.keys()):
        TreeEstimator = ALL_TREES[name]

        # test integer parameter
        est = TreeEstimator(
            min_samples_leaf=5, max_leaf_nodes=max_leaf_nodes, random_state=0
        )
        est.fit(X, y)
        out = est.tree_.apply(X)
        node_counts = np.bincount(out)
        # drop inner nodes
        leaf_count = node_counts[node_counts != 0]
        assert np.min(leaf_count) > 4, "Failed with {0}".format(name)

        # test float parameter
        est = TreeEstimator(
            min_samples_leaf=0.1, max_leaf_nodes=max_leaf_nodes, random_state=0
        )
        est.fit(X, y)
        out = est.tree_.apply(X)
        node_counts = np.bincount(out)
        # drop inner nodes
        leaf_count = node_counts[node_counts != 0]
        assert np.min(leaf_count) > 4, "Failed with {0}".format(name)


def check_min_weight_fraction_leaf(name, datasets, sparse_container=None):
    """Test if leaves contain at least min_weight_fraction_leaf of the
    training set"""
    X = DATASETS[datasets]["X"].astype(np.float32)
    if sparse_container is not None:
        X = sparse_container(X)
    y = DATASETS[datasets]["y"]

    weights = rng.rand(X.shape[0])
    total_weight = np.sum(weights)

    TreeEstimator = ALL_TREES[name]

    # test both DepthFirstTreeBuilder and BestFirstTreeBuilder
    # by setting max_leaf_nodes
    for max_leaf_nodes, frac in product((None, 1000), np.linspace(0, 0.5, 6)):
        est = TreeEstimator(
            min_weight_fraction_leaf=frac, max_leaf_nodes=max_leaf_nodes, random_state=0
        )
        est.fit(X, y, sample_weight=weights)

        if sparse_container is not None:
            out = est.tree_.apply(X.tocsr())
        else:
            out = est.tree_.apply(X)

        node_weights = np.bincount(out, weights=weights)
        # drop inner nodes
        leaf_weights = node_weights[node_weights != 0]
        assert (
            np.min(leaf_weights) >= total_weight * est.min_weight_fraction_leaf
        ), "Failed with {0} min_weight_fraction_leaf={1}".format(
            name, est.min_weight_fraction_leaf
        )

    # test case with no weights passed in
    total_weight = X.shape[0]

    for max_leaf_nodes, frac in product((None, 1000), np.linspace(0, 0.5, 6)):
        est = TreeEstimator(
            min_weight_fraction_leaf=frac, max_leaf_nodes=max_leaf_nodes, random_state=0
        )
        est.fit(X, y)

        if sparse_container is not None:
            out = est.tree_.apply(X.tocsr())
        else:
            out = est.tree_.apply(X)

        node_weights = np.bincount(out)
        # drop inner nodes
        leaf_weights = node_weights[node_weights != 0]
        assert (
            np.min(leaf_weights) >= total_weight * est.min_weight_fraction_leaf
        ), "Failed with {0} min_weight_fraction_leaf={1}".format(
            name, est.min_weight_fraction_leaf
        )


@pytest.mark.parametrize("name", ALL_TREES)
def test_min_weight_fraction_leaf_on_dense_input(name):
    check_min_weight_fraction_leaf(name, "iris")


@pytest.mark.parametrize("name", SPARSE_TREES)
@pytest.mark.parametrize("csc_container", CSC_CONTAINERS)
def test_min_weight_fraction_leaf_on_sparse_input(name, csc_container):
    check_min_weight_fraction_leaf(name, "multilabel", sparse_container=csc_container)


def check_min_weight_fraction_leaf_with_min_samples_leaf(
    name, datasets, sparse_container=None
):
    """Test the interaction between min_weight_fraction_leaf and
    min_samples_leaf when sample_weights is not provided in fit."""
    X = DATASETS[datasets]["X"].astype(np.float32)
    if sparse_container is not None:
        X = sparse_container(X)
    y = DATASETS[datasets]["y"]

    total_weight = X.shape[0]
    TreeEstimator = ALL_TREES[name]
    for max_leaf_nodes, frac in product((None, 1000), np.linspace(0, 0.5, 3)):
        # test integer min_samples_leaf
        est = TreeEstimator(
            min_weight_fraction_leaf=frac,
            max_leaf_nodes=max_leaf_nodes,
            min_samples_leaf=5,
            random_state=0,
        )
        est.fit(X, y)

        if sparse_container is not None:
            out = est.tree_.apply(X.tocsr())
        else:
            out = est.tree_.apply(X)

        node_weights = np.bincount(out)
        # drop inner nodes
        leaf_weights = node_weights[node_weights != 0]
        assert np.min(leaf_weights) >= max(
            (total_weight * est.min_weight_fraction_leaf), 5
        ), "Failed with {0} min_weight_fraction_leaf={1}, min_samples_leaf={2}".format(
            name, est.min_weight_fraction_leaf, est.min_samples_leaf
        )
    for max_leaf_nodes, frac in product((None, 1000), np.linspace(0, 0.5, 3)):
        # test float min_samples_leaf
        est = TreeEstimator(
            min_weight_fraction_leaf=frac,
            max_leaf_nodes=max_leaf_nodes,
            min_samples_leaf=0.1,
            random_state=0,
        )
        est.fit(X, y)

        if sparse_container is not None:
            out = est.tree_.apply(X.tocsr())
        else:
            out = est.tree_.apply(X)

        node_weights = np.bincount(out)
        # drop inner nodes
        leaf_weights = node_weights[node_weights != 0]
        assert np.min(leaf_weights) >= max(
            (total_weight * est.min_weight_fraction_leaf),
            (total_weight * est.min_samples_leaf),
        ), "Failed with {0} min_weight_fraction_leaf={1}, min_samples_leaf={2}".format(
            name, est.min_weight_fraction_leaf, est.min_samples_leaf
        )


@pytest.mark.parametrize("name", ALL_TREES)
def test_min_weight_fraction_leaf_with_min_samples_leaf_on_dense_input(name):
    check_min_weight_fraction_leaf_with_min_samples_leaf(name, "iris")


@pytest.mark.parametrize("name", SPARSE_TREES)
@pytest.mark.parametrize("csc_container", CSC_CONTAINERS)
def test_min_weight_fraction_leaf_with_min_samples_leaf_on_sparse_input(
    name, csc_container
):
    check_min_weight_fraction_leaf_with_min_samples_leaf(
        name, "multilabel", sparse_container=csc_container
    )


def test_min_impurity_decrease(global_random_seed):
    # test if min_impurity_decrease ensure that a split is made only if
    # if the impurity decrease is at least that value
    X, y = datasets.make_classification(n_samples=100, random_state=global_random_seed)

    # test both DepthFirstTreeBuilder and BestFirstTreeBuilder
    # by setting max_leaf_nodes
    for max_leaf_nodes, name in product((None, 1000), ALL_TREES.keys()):
        TreeEstimator = ALL_TREES[name]

        # Check default value of min_impurity_decrease, 1e-7
        est1 = TreeEstimator(max_leaf_nodes=max_leaf_nodes, random_state=0)
        # Check with explicit value of 0.05
        est2 = TreeEstimator(
            max_leaf_nodes=max_leaf_nodes, min_impurity_decrease=0.05, random_state=0
        )
        # Check with a much lower value of 0.0001
        est3 = TreeEstimator(
            max_leaf_nodes=max_leaf_nodes, min_impurity_decrease=0.0001, random_state=0
        )
        # Check with a much lower value of 0.1
        est4 = TreeEstimator(
            max_leaf_nodes=max_leaf_nodes, min_impurity_decrease=0.1, random_state=0
        )

        for est, expected_decrease in (
            (est1, 1e-7),
            (est2, 0.05),
            (est3, 0.0001),
            (est4, 0.1),
        ):
            assert (
                est.min_impurity_decrease <= expected_decrease
            ), "Failed, min_impurity_decrease = {0} > {1}".format(
                est.min_impurity_decrease, expected_decrease
            )
            est.fit(X, y)
            for node in range(est.tree_.node_count):
                # If current node is a not leaf node, check if the split was
                # justified w.r.t the min_impurity_decrease
                if est.tree_.children_left[node] != TREE_LEAF:
                    imp_parent = est.tree_.impurity[node]
                    wtd_n_node = est.tree_.weighted_n_node_samples[node]

                    left = est.tree_.children_left[node]
                    wtd_n_left = est.tree_.weighted_n_node_samples[left]
                    imp_left = est.tree_.impurity[left]
                    wtd_imp_left = wtd_n_left * imp_left

                    right = est.tree_.children_right[node]
                    wtd_n_right = est.tree_.weighted_n_node_samples[right]
                    imp_right = est.tree_.impurity[right]
                    wtd_imp_right = wtd_n_right * imp_right

                    wtd_avg_left_right_imp = wtd_imp_right + wtd_imp_left
                    wtd_avg_left_right_imp /= wtd_n_node

                    fractional_node_weight = (
                        est.tree_.weighted_n_node_samples[node] / X.shape[0]
                    )

                    actual_decrease = fractional_node_weight * (
                        imp_parent - wtd_avg_left_right_imp
                    )

                    assert (
                        actual_decrease >= expected_decrease
                    ), "Failed with {0} expected min_impurity_decrease={1}".format(
                        actual_decrease, expected_decrease
                    )


def test_pickle():
    """Test pickling preserves Tree properties and performance."""
    for name, TreeEstimator in ALL_TREES.items():
        if "Classifier" in name:
            X, y = iris.data, iris.target
        else:
            X, y = diabetes.data, diabetes.target

        est = TreeEstimator(random_state=0)
        est.fit(X, y)
        score = est.score(X, y)

        # test that all class properties are maintained
        attributes = [
            "max_depth",
            "node_count",
            "capacity",
            "n_classes",
            "children_left",
            "children_right",
            "n_leaves",
            "feature",
            "threshold",
            "impurity",
            "n_node_samples",
            "weighted_n_node_samples",
            "value",
        ]
        fitted_attribute = {
            attribute: getattr(est.tree_, attribute) for attribute in attributes
        }

        serialized_object = pickle.dumps(est)
        est2 = pickle.loads(serialized_object)
        assert type(est2) == est.__class__

        score2 = est2.score(X, y)
        assert (
            score == score2
        ), "Failed to generate same score  after pickling with {0}".format(name)
        for attribute in fitted_attribute:
            assert_array_equal(
                getattr(est2.tree_, attribute),
                fitted_attribute[attribute],
                err_msg=(
                    f"Failed to generate same attribute {attribute} after pickling with"
                    f" {name}"
                ),
            )


def test_multioutput():
    # Check estimators on multi-output problems.
    X = [
        [-2, -1],
        [-1, -1],
        [-1, -2],
        [1, 1],
        [1, 2],
        [2, 1],
        [-2, 1],
        [-1, 1],
        [-1, 2],
        [2, -1],
        [1, -1],
        [1, -2],
    ]

    y = [
        [-1, 0],
        [-1, 0],
        [-1, 0],
        [1, 1],
        [1, 1],
        [1, 1],
        [-1, 2],
        [-1, 2],
        [-1, 2],
        [1, 3],
        [1, 3],
        [1, 3],
    ]

    T = [[-1, -1], [1, 1], [-1, 1], [1, -1]]
    y_true = [[-1, 0], [1, 1], [-1, 2], [1, 3]]

    # toy classification problem
    for name, TreeClassifier in CLF_TREES.items():
        clf = TreeClassifier(random_state=0)
        y_hat = clf.fit(X, y).predict(T)
        assert_array_equal(y_hat, y_true)
        assert y_hat.shape == (4, 2)

        proba = clf.predict_proba(T)
        assert len(proba) == 2
        assert proba[0].shape == (4, 2)
        assert proba[1].shape == (4, 4)

        log_proba = clf.predict_log_proba(T)
        assert len(log_proba) == 2
        assert log_proba[0].shape == (4, 2)
        assert log_proba[1].shape == (4, 4)

    # toy regression problem
    for name, TreeRegressor in REG_TREES.items():
        reg = TreeRegressor(random_state=0)
        y_hat = reg.fit(X, y).predict(T)
        assert_almost_equal(y_hat, y_true)
        assert y_hat.shape == (4, 2)


def test_classes_shape():
    # Test that n_classes_ and classes_ have proper shape.
    for name, TreeClassifier in CLF_TREES.items():
        # Classification, single output
        clf = TreeClassifier(random_state=0)
        clf.fit(X, y)

        assert clf.n_classes_ == 2
        assert_array_equal(clf.classes_, [-1, 1])

        # Classification, multi-output
        _y = np.vstack((y, np.array(y) * 2)).T
        clf = TreeClassifier(random_state=0)
        clf.fit(X, _y)
        assert len(clf.n_classes_) == 2
        assert len(clf.classes_) == 2
        assert_array_equal(clf.n_classes_, [2, 2])
        assert_array_equal(clf.classes_, [[-1, 1], [-2, 2]])


def test_unbalanced_iris():
    # Check class rebalancing.
    unbalanced_X = iris.data[:125]
    unbalanced_y = iris.target[:125]
    sample_weight = compute_sample_weight("balanced", unbalanced_y)

    for name, TreeClassifier in CLF_TREES.items():
        clf = TreeClassifier(random_state=0)
        clf.fit(unbalanced_X, unbalanced_y, sample_weight=sample_weight)
        assert_almost_equal(clf.predict(unbalanced_X), unbalanced_y)


def test_memory_layout():
    # Check that it works no matter the memory layout
    for (name, TreeEstimator), dtype in product(
        ALL_TREES.items(), [np.float64, np.float32]
    ):
        est = TreeEstimator(random_state=0)

        # Nothing
        X = np.asarray(iris.data, dtype=dtype)
        y = iris.target
        assert_array_equal(est.fit(X, y).predict(X), y)

        # C-order
        X = np.asarray(iris.data, order="C", dtype=dtype)
        y = iris.target
        assert_array_equal(est.fit(X, y).predict(X), y)

        # F-order
        X = np.asarray(iris.data, order="F", dtype=dtype)
        y = iris.target
        assert_array_equal(est.fit(X, y).predict(X), y)

        # Contiguous
        X = np.ascontiguousarray(iris.data, dtype=dtype)
        y = iris.target
        assert_array_equal(est.fit(X, y).predict(X), y)

        # csr
        for csr_container in CSR_CONTAINERS:
            X = csr_container(iris.data, dtype=dtype)
            y = iris.target
            assert_array_equal(est.fit(X, y).predict(X), y)

        # csc
        for csc_container in CSC_CONTAINERS:
            X = csc_container(iris.data, dtype=dtype)
            y = iris.target
            assert_array_equal(est.fit(X, y).predict(X), y)

        # Strided
        X = np.asarray(iris.data[::3], dtype=dtype)
        y = iris.target[::3]
        assert_array_equal(est.fit(X, y).predict(X), y)


def test_sample_weight():
    # Check sample weighting.
    # Test that zero-weighted samples are not taken into account
    X = np.arange(100)[:, np.newaxis]
    y = np.ones(100)
    y[:50] = 0.0

    sample_weight = np.ones(100)
    sample_weight[y == 0] = 0.0

    clf = DecisionTreeClassifier(random_state=0)
    clf.fit(X, y, sample_weight=sample_weight)
    assert_array_equal(clf.predict(X), np.ones(100))

    # Test that low weighted samples are not taken into account at low depth
    X = np.arange(200)[:, np.newaxis]
    y = np.zeros(200)
    y[50:100] = 1
    y[100:200] = 2
    X[100:200, 0] = 200

    sample_weight = np.ones(200)

    sample_weight[y == 2] = 0.51  # Samples of class '2' are still weightier
    clf = DecisionTreeClassifier(max_depth=1, random_state=0)
    clf.fit(X, y, sample_weight=sample_weight)
    assert clf.tree_.threshold[0] == 149.5

    sample_weight[y == 2] = 0.5  # Samples of class '2' are no longer weightier
    clf = DecisionTreeClassifier(max_depth=1, random_state=0)
    clf.fit(X, y, sample_weight=sample_weight)
    assert clf.tree_.threshold[0] == 49.5  # Threshold should have moved

    # Test that sample weighting is the same as having duplicates
    X = iris.data
    y = iris.target

    duplicates = rng.randint(0, X.shape[0], 100)

    clf = DecisionTreeClassifier(random_state=1)
    clf.fit(X[duplicates], y[duplicates])

    sample_weight = np.bincount(duplicates, minlength=X.shape[0])
    clf2 = DecisionTreeClassifier(random_state=1)
    clf2.fit(X, y, sample_weight=sample_weight)

    internal = clf.tree_.children_left != tree._tree.TREE_LEAF
    assert_array_almost_equal(
        clf.tree_.threshold[internal], clf2.tree_.threshold[internal]
    )


def test_sample_weight_invalid():
    # Check sample weighting raises errors.
    X = np.arange(100)[:, np.newaxis]
    y = np.ones(100)
    y[:50] = 0.0

    clf = DecisionTreeClassifier(random_state=0)

    sample_weight = np.random.rand(100, 1)
    with pytest.raises(ValueError):
        clf.fit(X, y, sample_weight=sample_weight)

    sample_weight = np.array(0)
    expected_err = r"Singleton.* cannot be considered a valid collection"
    with pytest.raises(TypeError, match=expected_err):
        clf.fit(X, y, sample_weight=sample_weight)


@pytest.mark.parametrize("name", CLF_TREES)
def test_class_weights(name):
    # Test that class_weights resemble sample_weights behavior.
    TreeClassifier = CLF_TREES[name]

    # Iris is balanced, so no effect expected for using 'balanced' weights
    clf1 = TreeClassifier(random_state=0)
    clf1.fit(iris.data, iris.target)
    clf2 = TreeClassifier(class_weight="balanced", random_state=0)
    clf2.fit(iris.data, iris.target)
    assert_almost_equal(clf1.feature_importances_, clf2.feature_importances_)

    # Make a multi-output problem with three copies of Iris
    iris_multi = np.vstack((iris.target, iris.target, iris.target)).T
    # Create user-defined weights that should balance over the outputs
    clf3 = TreeClassifier(
        class_weight=[
            {0: 2.0, 1: 2.0, 2: 1.0},
            {0: 2.0, 1: 1.0, 2: 2.0},
            {0: 1.0, 1: 2.0, 2: 2.0},
        ],
        random_state=0,
    )
    clf3.fit(iris.data, iris_multi)
    assert_almost_equal(clf2.feature_importances_, clf3.feature_importances_)
    # Check against multi-output "auto" which should also have no effect
    clf4 = TreeClassifier(class_weight="balanced", random_state=0)
    clf4.fit(iris.data, iris_multi)
    assert_almost_equal(clf3.feature_importances_, clf4.feature_importances_)

    # Inflate importance of class 1, check against user-defined weights
    sample_weight = np.ones(iris.target.shape)
    sample_weight[iris.target == 1] *= 100
    class_weight = {0: 1.0, 1: 100.0, 2: 1.0}
    clf1 = TreeClassifier(random_state=0)
    clf1.fit(iris.data, iris.target, sample_weight)
    clf2 = TreeClassifier(class_weight=class_weight, random_state=0)
    clf2.fit(iris.data, iris.target)
    assert_almost_equal(clf1.feature_importances_, clf2.feature_importances_)

    # Check that sample_weight and class_weight are multiplicative
    clf1 = TreeClassifier(random_state=0)
    clf1.fit(iris.data, iris.target, sample_weight**2)
    clf2 = TreeClassifier(class_weight=class_weight, random_state=0)
    clf2.fit(iris.data, iris.target, sample_weight)
    assert_almost_equal(clf1.feature_importances_, clf2.feature_importances_)


@pytest.mark.parametrize("name", CLF_TREES)
def test_class_weight_errors(name):
    # Test if class_weight raises errors and warnings when expected.
    TreeClassifier = CLF_TREES[name]
    _y = np.vstack((y, np.array(y) * 2)).T

    # Incorrect length list for multi-output
    clf = TreeClassifier(class_weight=[{-1: 0.5, 1: 1.0}], random_state=0)
    err_msg = "number of elements in class_weight should match number of outputs."
    with pytest.raises(ValueError, match=err_msg):
        clf.fit(X, _y)


def test_max_leaf_nodes():
    # Test greedy trees with max_depth + 1 leafs.
    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)
    k = 4
    for name, TreeEstimator in ALL_TREES.items():
        est = TreeEstimator(max_depth=None, max_leaf_nodes=k + 1).fit(X, y)
        assert est.get_n_leaves() == k + 1


def test_max_leaf_nodes_max_depth():
    # Test precedence of max_leaf_nodes over max_depth.
    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)
    k = 4
    for name, TreeEstimator in ALL_TREES.items():
        est = TreeEstimator(max_depth=1, max_leaf_nodes=k).fit(X, y)
        assert est.get_depth() == 1


def test_arrays_persist():
    # Ensure property arrays' memory stays alive when tree disappears
    # non-regression for #2726
    for attr in [
        "n_classes",
        "value",
        "children_left",
        "children_right",
        "threshold",
        "impurity",
        "feature",
        "n_node_samples",
    ]:
        value = getattr(DecisionTreeClassifier().fit([[0], [1]], [0, 1]).tree_, attr)
        # if pointing to freed memory, contents may be arbitrary
        assert -3 <= value.flat[0] < 3, "Array points to arbitrary memory"


def test_only_constant_features():
    random_state = check_random_state(0)
    X = np.zeros((10, 20))
    y = random_state.randint(0, 2, (10,))
    for name, TreeEstimator in ALL_TREES.items():
        est = TreeEstimator(random_state=0)
        est.fit(X, y)
        assert est.tree_.max_depth == 0


def test_behaviour_constant_feature_after_splits():
    X = np.transpose(
        np.vstack(([[0, 0, 0, 0, 0, 1, 2, 4, 5, 6, 7]], np.zeros((4, 11))))
    )
    y = [0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 3]
    for name, TreeEstimator in ALL_TREES.items():
        # do not check extra random trees
        if "ExtraTree" not in name:
            est = TreeEstimator(random_state=0, max_features=1)
            est.fit(X, y)
            assert est.tree_.max_depth == 2
            assert est.tree_.node_count == 5


def test_with_only_one_non_constant_features():
    X = np.hstack([np.array([[1.0], [1.0], [0.0], [0.0]]), np.zeros((4, 1000))])

    y = np.array([0.0, 1.0, 0.0, 1.0])
    for name, TreeEstimator in CLF_TREES.items():
        est = TreeEstimator(random_state=0, max_features=1)
        est.fit(X, y)
        assert est.tree_.max_depth == 1
        assert_array_equal(est.predict_proba(X), np.full((4, 2), 0.5))

    for name, TreeEstimator in REG_TREES.items():
        est = TreeEstimator(random_state=0, max_features=1)
        est.fit(X, y)
        assert est.tree_.max_depth == 1
        assert_array_equal(est.predict(X), np.full((4,), 0.5))


def test_big_input():
    # Test if the warning for too large inputs is appropriate.
    X = np.repeat(10**40.0, 4).astype(np.float64).reshape(-1, 1)
    clf = DecisionTreeClassifier()
    with pytest.raises(ValueError, match="float32"):
        clf.fit(X, [0, 1, 0, 1])


def test_realloc():
    from sklearn.tree._utils import _realloc_test

    with pytest.raises(MemoryError):
        _realloc_test()


def test_huge_allocations():
    n_bits = 8 * struct.calcsize("P")

    X = np.random.randn(10, 2)
    y = np.random.randint(0, 2, 10)

    # Sanity check: we cannot request more memory than the size of the address
    # space. Currently raises OverflowError.
    huge = 2 ** (n_bits + 1)
    clf = DecisionTreeClassifier(splitter="best", max_leaf_nodes=huge)
    with pytest.raises(Exception):
        clf.fit(X, y)

    # Non-regression test: MemoryError used to be dropped by Cython
    # because of missing "except *".
    huge = 2 ** (n_bits - 1) - 1
    clf = DecisionTreeClassifier(splitter="best", max_leaf_nodes=huge)
    with pytest.raises(MemoryError):
        clf.fit(X, y)


def check_sparse_input(tree, dataset, max_depth=None):
    TreeEstimator = ALL_TREES[tree]
    X = DATASETS[dataset]["X"]
    y = DATASETS[dataset]["y"]

    # Gain testing time
    if dataset in ["digits", "diabetes"]:
        n_samples = X.shape[0] // 5
        X = X[:n_samples]
        y = y[:n_samples]

    for sparse_container in COO_CONTAINERS + CSC_CONTAINERS + CSR_CONTAINERS:
        X_sparse = sparse_container(X)

        # Check the default (depth first search)
        d = TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y)
        s = TreeEstimator(random_state=0, max_depth=max_depth).fit(X_sparse, y)

        assert_tree_equal(
            d.tree_,
            s.tree_,
            "{0} with dense and sparse format gave different trees".format(tree),
        )

        y_pred = d.predict(X)
        if tree in CLF_TREES:
            y_proba = d.predict_proba(X)
            y_log_proba = d.predict_log_proba(X)

        for sparse_container_test in COO_CONTAINERS + CSR_CONTAINERS + CSC_CONTAINERS:
            X_sparse_test = sparse_container_test(X_sparse, dtype=np.float32)

            assert_array_almost_equal(s.predict(X_sparse_test), y_pred)

            if tree in CLF_TREES:
                assert_array_almost_equal(s.predict_proba(X_sparse_test), y_proba)
                assert_array_almost_equal(
                    s.predict_log_proba(X_sparse_test), y_log_proba
                )


@pytest.mark.parametrize("tree_type", SPARSE_TREES)
@pytest.mark.parametrize(
    "dataset",
    (
        "clf_small",
        "toy",
        "digits",
        "multilabel",
        "sparse-pos",
        "sparse-neg",
        "sparse-mix",
        "zeros",
    ),
)
def test_sparse_input(tree_type, dataset):
    max_depth = 3 if dataset == "digits" else None
    check_sparse_input(tree_type, dataset, max_depth)


@pytest.mark.parametrize("tree_type", sorted(set(SPARSE_TREES).intersection(REG_TREES)))
@pytest.mark.parametrize("dataset", ["diabetes", "reg_small"])
def test_sparse_input_reg_trees(tree_type, dataset):
    # Due to numerical instability of MSE and too strict test, we limit the
    # maximal depth
    check_sparse_input(tree_type, dataset, 2)


@pytest.mark.parametrize("tree_type", SPARSE_TREES)
@pytest.mark.parametrize("dataset", ["sparse-pos", "sparse-neg", "sparse-mix", "zeros"])
@pytest.mark.parametrize("csc_container", CSC_CONTAINERS)
def test_sparse_parameters(tree_type, dataset, csc_container):
    TreeEstimator = ALL_TREES[tree_type]
    X = DATASETS[dataset]["X"]
    X_sparse = csc_container(X)
    y = DATASETS[dataset]["y"]

    # Check max_features
    d = TreeEstimator(random_state=0, max_features=1, max_depth=2).fit(X, y)
    s = TreeEstimator(random_state=0, max_features=1, max_depth=2).fit(X_sparse, y)
    assert_tree_equal(
        d.tree_,
        s.tree_,
        "{0} with dense and sparse format gave different trees".format(tree_type),
    )
    assert_array_almost_equal(s.predict(X), d.predict(X))

    # Check min_samples_split
    d = TreeEstimator(random_state=0, max_features=1, min_samples_split=10).fit(X, y)
    s = TreeEstimator(random_state=0, max_features=1, min_samples_split=10).fit(
        X_sparse, y
    )
    assert_tree_equal(
        d.tree_,
        s.tree_,
        "{0} with dense and sparse format gave different trees".format(tree_type),
    )
    assert_array_almost_equal(s.predict(X), d.predict(X))

    # Check min_samples_leaf
    d = TreeEstimator(random_state=0, min_samples_leaf=X_sparse.shape[0] // 2).fit(X, y)
    s = TreeEstimator(random_state=0, min_samples_leaf=X_sparse.shape[0] // 2).fit(
        X_sparse, y
    )
    assert_tree_equal(
        d.tree_,
        s.tree_,
        "{0} with dense and sparse format gave different trees".format(tree_type),
    )
    assert_array_almost_equal(s.predict(X), d.predict(X))

    # Check best-first search
    d = TreeEstimator(random_state=0, max_leaf_nodes=3).fit(X, y)
    s = TreeEstimator(random_state=0, max_leaf_nodes=3).fit(X_sparse, y)
    assert_tree_equal(
        d.tree_,
        s.tree_,
        "{0} with dense and sparse format gave different trees".format(tree_type),
    )
    assert_array_almost_equal(s.predict(X), d.predict(X))


@pytest.mark.parametrize(
    "tree_type, criterion",
    list(product([tree for tree in SPARSE_TREES if tree in REG_TREES], REG_CRITERIONS))
    + list(
        product([tree for tree in SPARSE_TREES if tree in CLF_TREES], CLF_CRITERIONS)
    ),
)
@pytest.mark.parametrize("dataset", ["sparse-pos", "sparse-neg", "sparse-mix", "zeros"])
@pytest.mark.parametrize("csc_container", CSC_CONTAINERS)
def test_sparse_criteria(tree_type, dataset, csc_container, criterion):
    TreeEstimator = ALL_TREES[tree_type]
    X = DATASETS[dataset]["X"]
    X_sparse = csc_container(X)
    y = DATASETS[dataset]["y"]

    d = TreeEstimator(random_state=0, max_depth=3, criterion=criterion).fit(X, y)
    s = TreeEstimator(random_state=0, max_depth=3, criterion=criterion).fit(X_sparse, y)

    assert_tree_equal(
        d.tree_,
        s.tree_,
        "{0} with dense and sparse format gave different trees".format(tree_type),
    )
    assert_array_almost_equal(s.predict(X), d.predict(X))


@pytest.mark.parametrize("tree_type", SPARSE_TREES)
@pytest.mark.parametrize(
    "csc_container,csr_container", zip(CSC_CONTAINERS, CSR_CONTAINERS)
)
def test_explicit_sparse_zeros(tree_type, csc_container, csr_container):
    TreeEstimator = ALL_TREES[tree_type]
    max_depth = 3
    n_features = 10

    # n_samples set n_feature to ease construction of a simultaneous
    # construction of a csr and csc matrix
    n_samples = n_features
    samples = np.arange(n_samples)

    # Generate X, y
    random_state = check_random_state(0)
    indices = []
    data = []
    offset = 0
    indptr = [offset]
    for i in range(n_features):
        n_nonzero_i = random_state.binomial(n_samples, 0.5)
        indices_i = random_state.permutation(samples)[:n_nonzero_i]
        indices.append(indices_i)
        data_i = random_state.binomial(3, 0.5, size=(n_nonzero_i,)) - 1
        data.append(data_i)
        offset += n_nonzero_i
        indptr.append(offset)

    indices = np.concatenate(indices).astype(np.int32)
    indptr = np.array(indptr, dtype=np.int32)
    data = np.array(np.concatenate(data), dtype=np.float32)
    X_sparse = csc_container((data, indices, indptr), shape=(n_samples, n_features))
    X = X_sparse.toarray()
    X_sparse_test = csr_container(
        (data, indices, indptr), shape=(n_samples, n_features)
    )
    X_test = X_sparse_test.toarray()
    y = random_state.randint(0, 3, size=(n_samples,))

    # Ensure that X_sparse_test owns its data, indices and indptr array
    X_sparse_test = X_sparse_test.copy()

    # Ensure that we have explicit zeros
    assert (X_sparse.data == 0.0).sum() > 0
    assert (X_sparse_test.data == 0.0).sum() > 0

    # Perform the comparison
    d = TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y)
    s = TreeEstimator(random_state=0, max_depth=max_depth).fit(X_sparse, y)

    assert_tree_equal(
        d.tree_,
        s.tree_,
        "{0} with dense and sparse format gave different trees".format(tree),
    )

    Xs = (X_test, X_sparse_test)
    for X1, X2 in product(Xs, Xs):
        assert_array_almost_equal(s.tree_.apply(X1), d.tree_.apply(X2))
        assert_array_almost_equal(s.apply(X1), d.apply(X2))
        assert_array_almost_equal(s.apply(X1), s.tree_.apply(X1))

        assert_array_almost_equal(
            s.tree_.decision_path(X1).toarray(), d.tree_.decision_path(X2).toarray()
        )
        assert_array_almost_equal(
            s.decision_path(X1).toarray(), d.decision_path(X2).toarray()
        )
        assert_array_almost_equal(
            s.decision_path(X1).toarray(), s.tree_.decision_path(X1).toarray()
        )

        assert_array_almost_equal(s.predict(X1), d.predict(X2))

        if tree in CLF_TREES:
            assert_array_almost_equal(s.predict_proba(X1), d.predict_proba(X2))


@ignore_warnings
def check_raise_error_on_1d_input(name):
    TreeEstimator = ALL_TREES[name]

    X = iris.data[:, 0].ravel()
    X_2d = iris.data[:, 0].reshape((-1, 1))
    y = iris.target

    with pytest.raises(ValueError):
        TreeEstimator(random_state=0).fit(X, y)

    est = TreeEstimator(random_state=0)
    est.fit(X_2d, y)
    with pytest.raises(ValueError):
        est.predict([X])


@pytest.mark.parametrize("name", ALL_TREES)
def test_1d_input(name):
    with ignore_warnings():
        check_raise_error_on_1d_input(name)


@pytest.mark.parametrize("name", ALL_TREES)
@pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS)
def test_min_weight_leaf_split_level(name, sparse_container):
    TreeEstimator = ALL_TREES[name]

    X = np.array([[0], [0], [0], [0], [1]])
    y = [0, 0, 0, 0, 1]
    sample_weight = [0.2, 0.2, 0.2, 0.2, 0.2]
    if sparse_container is not None:
        X = sparse_container(X)

    est = TreeEstimator(random_state=0)
    est.fit(X, y, sample_weight=sample_weight)
    assert est.tree_.max_depth == 1

    est = TreeEstimator(random_state=0, min_weight_fraction_leaf=0.4)
    est.fit(X, y, sample_weight=sample_weight)
    assert est.tree_.max_depth == 0


@pytest.mark.parametrize("name", ALL_TREES)
def test_public_apply_all_trees(name):
    X_small32 = X_small.astype(tree._tree.DTYPE, copy=False)

    est = ALL_TREES[name]()
    est.fit(X_small, y_small)
    assert_array_equal(est.apply(X_small), est.tree_.apply(X_small32))


@pytest.mark.parametrize("name", SPARSE_TREES)
@pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
def test_public_apply_sparse_trees(name, csr_container):
    X_small32 = csr_container(X_small.astype(tree._tree.DTYPE, copy=False))

    est = ALL_TREES[name]()
    est.fit(X_small, y_small)
    assert_array_equal(est.apply(X_small), est.tree_.apply(X_small32))


def test_decision_path_hardcoded():
    X = iris.data
    y = iris.target
    est = DecisionTreeClassifier(random_state=0, max_depth=1).fit(X, y)
    node_indicator = est.decision_path(X[:2]).toarray()
    assert_array_equal(node_indicator, [[1, 1, 0], [1, 0, 1]])


@pytest.mark.parametrize("name", ALL_TREES)
def test_decision_path(name):
    X = iris.data
    y = iris.target
    n_samples = X.shape[0]

    TreeEstimator = ALL_TREES[name]
    est = TreeEstimator(random_state=0, max_depth=2)
    est.fit(X, y)

    node_indicator_csr = est.decision_path(X)
    node_indicator = node_indicator_csr.toarray()
    assert node_indicator.shape == (n_samples, est.tree_.node_count)

    # Assert that leaves index are correct
    leaves = est.apply(X)
    leave_indicator = [node_indicator[i, j] for i, j in enumerate(leaves)]
    assert_array_almost_equal(leave_indicator, np.ones(shape=n_samples))

    # Ensure only one leave node per sample
    all_leaves = est.tree_.children_left == TREE_LEAF
    assert_array_almost_equal(
        np.dot(node_indicator, all_leaves), np.ones(shape=n_samples)
    )

    # Ensure max depth is consistent with sum of indicator
    max_depth = node_indicator.sum(axis=1).max()
    assert est.tree_.max_depth <= max_depth


@pytest.mark.parametrize("name", ALL_TREES)
@pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
def test_no_sparse_y_support(name, csr_container):
    # Currently we don't support sparse y
    X, y = X_multilabel, csr_container(y_multilabel)
    TreeEstimator = ALL_TREES[name]
    with pytest.raises(TypeError):
        TreeEstimator(random_state=0).fit(X, y)


def test_mae():
    """Check MAE criterion produces correct results on small toy dataset:

    ------------------
    | X | y | weight |
    ------------------
    | 3 | 3 |  0.1   |
    | 5 | 3 |  0.3   |
    | 8 | 4 |  1.0   |
    | 3 | 6 |  0.6   |
    | 5 | 7 |  0.3   |
    ------------------
    |sum wt:|  2.3   |
    ------------------

    Because we are dealing with sample weights, we cannot find the median by
    simply choosing/averaging the centre value(s), instead we consider the
    median where 50% of the cumulative weight is found (in a y sorted data set)
    . Therefore with regards to this test data, the cumulative weight is >= 50%
    when y = 4.  Therefore:
    Median = 4

    For all the samples, we can get the total error by summing:
    Absolute(Median - y) * weight

    I.e., total error = (Absolute(4 - 3) * 0.1)
                      + (Absolute(4 - 3) * 0.3)
                      + (Absolute(4 - 4) * 1.0)
                      + (Absolute(4 - 6) * 0.6)
                      + (Absolute(4 - 7) * 0.3)
                      = 2.5

    Impurity = Total error / total weight
             = 2.5 / 2.3
             = 1.08695652173913
             ------------------

    From this root node, the next best split is between X values of 3 and 5.
    Thus, we have left and right child nodes:

    LEFT                    RIGHT
    ------------------      ------------------
    | X | y | weight |      | X | y | weight |
    ------------------      ------------------
    | 3 | 3 |  0.1   |      | 5 | 3 |  0.3   |
    | 3 | 6 |  0.6   |      | 8 | 4 |  1.0   |
    ------------------      | 5 | 7 |  0.3   |
    |sum wt:|  0.7   |      ------------------
    ------------------      |sum wt:|  1.6   |
                            ------------------

    Impurity is found in the same way:
    Left node Median = 6
    Total error = (Absolute(6 - 3) * 0.1)
                + (Absolute(6 - 6) * 0.6)
                = 0.3

    Left Impurity = Total error / total weight
            = 0.3 / 0.7
            = 0.428571428571429
            -------------------

    Likewise for Right node:
    Right node Median = 4
    Total error = (Absolute(4 - 3) * 0.3)
                + (Absolute(4 - 4) * 1.0)
                + (Absolute(4 - 7) * 0.3)
                = 1.2

    Right Impurity = Total error / total weight
            = 1.2 / 1.6
            = 0.75
            ------
    """
    dt_mae = DecisionTreeRegressor(
        random_state=0, criterion="absolute_error", max_leaf_nodes=2
    )

    # Test MAE where sample weights are non-uniform (as illustrated above):
    dt_mae.fit(
        X=[[3], [5], [3], [8], [5]],
        y=[6, 7, 3, 4, 3],
        sample_weight=[0.6, 0.3, 0.1, 1.0, 0.3],
    )
    assert_allclose(dt_mae.tree_.impurity, [2.5 / 2.3, 0.3 / 0.7, 1.2 / 1.6])
    assert_array_equal(dt_mae.tree_.value.flat, [4.0, 6.0, 4.0])

    # Test MAE where all sample weights are uniform:
    dt_mae.fit(X=[[3], [5], [3], [8], [5]], y=[6, 7, 3, 4, 3], sample_weight=np.ones(5))
    assert_array_equal(dt_mae.tree_.impurity, [1.4, 1.5, 4.0 / 3.0])
    assert_array_equal(dt_mae.tree_.value.flat, [4, 4.5, 4.0])

    # Test MAE where a `sample_weight` is not explicitly provided.
    # This is equivalent to providing uniform sample weights, though
    # the internal logic is different:
    dt_mae.fit(X=[[3], [5], [3], [8], [5]], y=[6, 7, 3, 4, 3])
    assert_array_equal(dt_mae.tree_.impurity, [1.4, 1.5, 4.0 / 3.0])
    assert_array_equal(dt_mae.tree_.value.flat, [4, 4.5, 4.0])


def test_criterion_copy():
    # Let's check whether copy of our criterion has the same type
    # and properties as original
    n_outputs = 3
    n_classes = np.arange(3, dtype=np.intp)
    n_samples = 100

    def _pickle_copy(obj):
        return pickle.loads(pickle.dumps(obj))

    for copy_func in [copy.copy, copy.deepcopy, _pickle_copy]:
        for _, typename in CRITERIA_CLF.items():
            criteria = typename(n_outputs, n_classes)
            result = copy_func(criteria).__reduce__()
            typename_, (n_outputs_, n_classes_), _ = result
            assert typename == typename_
            assert n_outputs == n_outputs_
            assert_array_equal(n_classes, n_classes_)

        for _, typename in CRITERIA_REG.items():
            criteria = typename(n_outputs, n_samples)
            result = copy_func(criteria).__reduce__()
            typename_, (n_outputs_, n_samples_), _ = result
            assert typename == typename_
            assert n_outputs == n_outputs_
            assert n_samples == n_samples_


@pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS)
def test_empty_leaf_infinite_threshold(sparse_container):
    # try to make empty leaf by using near infinite value.
    data = np.random.RandomState(0).randn(100, 11) * 2e38
    data = np.nan_to_num(data.astype("float32"))
    X = data[:, :-1]
    if sparse_container is not None:
        X = sparse_container(X)
    y = data[:, -1]

    tree = DecisionTreeRegressor(random_state=0).fit(X, y)
    terminal_regions = tree.apply(X)
    left_leaf = set(np.where(tree.tree_.children_left == TREE_LEAF)[0])
    empty_leaf = left_leaf.difference(terminal_regions)
    infinite_threshold = np.where(~np.isfinite(tree.tree_.threshold))[0]
    assert len(infinite_threshold) == 0
    assert len(empty_leaf) == 0


@pytest.mark.parametrize(
    "dataset", sorted(set(DATASETS.keys()) - {"reg_small", "diabetes"})
)
@pytest.mark.parametrize("tree_cls", [DecisionTreeClassifier, ExtraTreeClassifier])
def test_prune_tree_classifier_are_subtrees(dataset, tree_cls):
    dataset = DATASETS[dataset]
    X, y = dataset["X"], dataset["y"]
    est = tree_cls(max_leaf_nodes=20, random_state=0)
    info = est.cost_complexity_pruning_path(X, y)

    pruning_path = info.ccp_alphas
    impurities = info.impurities
    assert np.all(np.diff(pruning_path) >= 0)
    assert np.all(np.diff(impurities) >= 0)

    assert_pruning_creates_subtree(tree_cls, X, y, pruning_path)


@pytest.mark.parametrize("dataset", DATASETS.keys())
@pytest.mark.parametrize("tree_cls", [DecisionTreeRegressor, ExtraTreeRegressor])
def test_prune_tree_regression_are_subtrees(dataset, tree_cls):
    dataset = DATASETS[dataset]
    X, y = dataset["X"], dataset["y"]

    est = tree_cls(max_leaf_nodes=20, random_state=0)
    info = est.cost_complexity_pruning_path(X, y)

    pruning_path = info.ccp_alphas
    impurities = info.impurities
    assert np.all(np.diff(pruning_path) >= 0)
    assert np.all(np.diff(impurities) >= 0)

    assert_pruning_creates_subtree(tree_cls, X, y, pruning_path)


def test_prune_single_node_tree():
    # single node tree
    clf1 = DecisionTreeClassifier(random_state=0)
    clf1.fit([[0], [1]], [0, 0])

    # pruned single node tree
    clf2 = DecisionTreeClassifier(random_state=0, ccp_alpha=10)
    clf2.fit([[0], [1]], [0, 0])

    assert_is_subtree(clf1.tree_, clf2.tree_)


def assert_pruning_creates_subtree(estimator_cls, X, y, pruning_path):
    # generate trees with increasing alphas
    estimators = []
    for ccp_alpha in pruning_path:
        est = estimator_cls(max_leaf_nodes=20, ccp_alpha=ccp_alpha, random_state=0).fit(
            X, y
        )
        estimators.append(est)

    # A pruned tree must be a subtree of the previous tree (which had a
    # smaller ccp_alpha)
    for prev_est, next_est in zip(estimators, estimators[1:]):
        assert_is_subtree(prev_est.tree_, next_est.tree_)


def assert_is_subtree(tree, subtree):
    assert tree.node_count >= subtree.node_count
    assert tree.max_depth >= subtree.max_depth

    tree_c_left = tree.children_left
    tree_c_right = tree.children_right
    subtree_c_left = subtree.children_left
    subtree_c_right = subtree.children_right

    stack = [(0, 0)]
    while stack:
        tree_node_idx, subtree_node_idx = stack.pop()
        assert_array_almost_equal(
            tree.value[tree_node_idx], subtree.value[subtree_node_idx]
        )
        assert_almost_equal(
            tree.impurity[tree_node_idx], subtree.impurity[subtree_node_idx]
        )
        assert_almost_equal(
            tree.n_node_samples[tree_node_idx], subtree.n_node_samples[subtree_node_idx]
        )
        assert_almost_equal(
            tree.weighted_n_node_samples[tree_node_idx],
            subtree.weighted_n_node_samples[subtree_node_idx],
        )

        if subtree_c_left[subtree_node_idx] == subtree_c_right[subtree_node_idx]:
            # is a leaf
            assert_almost_equal(TREE_UNDEFINED, subtree.threshold[subtree_node_idx])
        else:
            # not a leaf
            assert_almost_equal(
                tree.threshold[tree_node_idx], subtree.threshold[subtree_node_idx]
            )
            stack.append((tree_c_left[tree_node_idx], subtree_c_left[subtree_node_idx]))
            stack.append(
                (tree_c_right[tree_node_idx], subtree_c_right[subtree_node_idx])
            )


@pytest.mark.parametrize("name", ALL_TREES)
@pytest.mark.parametrize("splitter", ["best", "random"])
@pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
def test_apply_path_readonly_all_trees(name, splitter, sparse_container):
    dataset = DATASETS["clf_small"]
    X_small = dataset["X"].astype(tree._tree.DTYPE, copy=False)
    if sparse_container is None:
        X_readonly = create_memmap_backed_data(X_small)
    else:
        X_readonly = sparse_container(dataset["X"])

        X_readonly.data = np.array(X_readonly.data, dtype=tree._tree.DTYPE)
        (
            X_readonly.data,
            X_readonly.indices,
            X_readonly.indptr,
        ) = create_memmap_backed_data(
            (X_readonly.data, X_readonly.indices, X_readonly.indptr)
        )

    y_readonly = create_memmap_backed_data(np.array(y_small, dtype=tree._tree.DTYPE))
    est = ALL_TREES[name](splitter=splitter)
    est.fit(X_readonly, y_readonly)
    assert_array_equal(est.predict(X_readonly), est.predict(X_small))
    assert_array_equal(
        est.decision_path(X_readonly).todense(), est.decision_path(X_small).todense()
    )


@pytest.mark.parametrize("criterion", ["squared_error", "friedman_mse", "poisson"])
@pytest.mark.parametrize("Tree", REG_TREES.values())
def test_balance_property(criterion, Tree):
    # Test that sum(y_pred)=sum(y_true) on training set.
    # This works if the mean is predicted (should even be true for each leaf).
    # MAE predicts the median and is therefore excluded from this test.

    # Choose a training set with non-negative targets (for poisson)
    X, y = diabetes.data, diabetes.target
    reg = Tree(criterion=criterion)
    reg.fit(X, y)
    assert np.sum(reg.predict(X)) == pytest.approx(np.sum(y))


@pytest.mark.parametrize("seed", range(3))
def test_poisson_zero_nodes(seed):
    # Test that sum(y)=0 and therefore y_pred=0 is forbidden on nodes.
    X = [[0, 0], [0, 1], [0, 2], [0, 3], [1, 0], [1, 2], [1, 2], [1, 3]]
    y = [0, 0, 0, 0, 1, 2, 3, 4]
    # Note that X[:, 0] == 0 is a 100% indicator for y == 0. The tree can
    # easily learn that:
    reg = DecisionTreeRegressor(criterion="squared_error", random_state=seed)
    reg.fit(X, y)
    assert np.amin(reg.predict(X)) == 0
    # whereas Poisson must predict strictly positive numbers
    reg = DecisionTreeRegressor(criterion="poisson", random_state=seed)
    reg.fit(X, y)
    assert np.all(reg.predict(X) > 0)

    # Test additional dataset where something could go wrong.
    n_features = 10
    X, y = datasets.make_regression(
        effective_rank=n_features * 2 // 3,
        tail_strength=0.6,
        n_samples=1_000,
        n_features=n_features,
        n_informative=n_features * 2 // 3,
        random_state=seed,
    )
    # some excess zeros
    y[(-1 < y) & (y < 0)] = 0
    # make sure the target is positive
    y = np.abs(y)
    reg = DecisionTreeRegressor(criterion="poisson", random_state=seed)
    reg.fit(X, y)
    assert np.all(reg.predict(X) > 0)


def test_poisson_vs_mse():
    # For a Poisson distributed target, Poisson loss should give better results
    # than squared error measured in Poisson deviance as metric.
    # We have a similar test, test_poisson(), in
    # sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py
    rng = np.random.RandomState(42)
    n_train, n_test, n_features = 500, 500, 10
    X = datasets.make_low_rank_matrix(
        n_samples=n_train + n_test, n_features=n_features, random_state=rng
    )
    # We create a log-linear Poisson model and downscale coef as it will get
    # exponentiated.
    coef = rng.uniform(low=-2, high=2, size=n_features) / np.max(X, axis=0)
    y = rng.poisson(lam=np.exp(X @ coef))
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=n_test, random_state=rng
    )
    # We prevent some overfitting by setting min_samples_split=10.
    tree_poi = DecisionTreeRegressor(
        criterion="poisson", min_samples_split=10, random_state=rng
    )
    tree_mse = DecisionTreeRegressor(
        criterion="squared_error", min_samples_split=10, random_state=rng
    )

    tree_poi.fit(X_train, y_train)
    tree_mse.fit(X_train, y_train)
    dummy = DummyRegressor(strategy="mean").fit(X_train, y_train)

    for X, y, val in [(X_train, y_train, "train"), (X_test, y_test, "test")]:
        metric_poi = mean_poisson_deviance(y, tree_poi.predict(X))
        # squared_error might produce non-positive predictions => clip
        metric_mse = mean_poisson_deviance(y, np.clip(tree_mse.predict(X), 1e-15, None))
        metric_dummy = mean_poisson_deviance(y, dummy.predict(X))
        # As squared_error might correctly predict 0 in train set, its train
        # score can be better than Poisson. This is no longer the case for the
        # test set.
        if val == "test":
            assert metric_poi < 0.5 * metric_mse
        assert metric_poi < 0.75 * metric_dummy


@pytest.mark.parametrize("criterion", REG_CRITERIONS)
def test_decision_tree_regressor_sample_weight_consistency(criterion):
    """Test that the impact of sample_weight is consistent."""
    tree_params = dict(criterion=criterion)
    tree = DecisionTreeRegressor(**tree_params, random_state=42)
    for kind in ["zeros", "ones"]:
        check_sample_weights_invariance(
            "DecisionTreeRegressor_" + criterion, tree, kind="zeros"
        )

    rng = np.random.RandomState(0)
    n_samples, n_features = 10, 5

    X = rng.rand(n_samples, n_features)
    y = np.mean(X, axis=1) + rng.rand(n_samples)
    # make it positive in order to work also for poisson criterion
    y += np.min(y) + 0.1

    # check that multiplying sample_weight by 2 is equivalent
    # to repeating corresponding samples twice
    X2 = np.concatenate([X, X[: n_samples // 2]], axis=0)
    y2 = np.concatenate([y, y[: n_samples // 2]])
    sample_weight_1 = np.ones(len(y))
    sample_weight_1[: n_samples // 2] = 2

    tree1 = DecisionTreeRegressor(**tree_params).fit(
        X, y, sample_weight=sample_weight_1
    )

    tree2 = DecisionTreeRegressor(**tree_params).fit(X2, y2, sample_weight=None)

    assert tree1.tree_.node_count == tree2.tree_.node_count
    # Thresholds, tree.tree_.threshold, and values, tree.tree_.value, are not
    # exactly the same, but on the training set, those differences do not
    # matter and thus predictions are the same.
    assert_allclose(tree1.predict(X), tree2.predict(X))


@pytest.mark.parametrize("Tree", [DecisionTreeClassifier, ExtraTreeClassifier])
@pytest.mark.parametrize("n_classes", [2, 4])
def test_criterion_entropy_same_as_log_loss(Tree, n_classes):
    """Test that criterion=entropy gives same as log_loss."""
    n_samples, n_features = 50, 5
    X, y = datasets.make_classification(
        n_classes=n_classes,
        n_samples=n_samples,
        n_features=n_features,
        n_informative=n_features,
        n_redundant=0,
        random_state=42,
    )
    tree_log_loss = Tree(criterion="log_loss", random_state=43).fit(X, y)
    tree_entropy = Tree(criterion="entropy", random_state=43).fit(X, y)

    assert_tree_equal(
        tree_log_loss.tree_,
        tree_entropy.tree_,
        f"{Tree!r} with criterion 'entropy' and 'log_loss' gave different trees.",
    )
    assert_allclose(tree_log_loss.predict(X), tree_entropy.predict(X))


def test_different_endianness_pickle():
    X, y = datasets.make_classification(random_state=0)

    clf = DecisionTreeClassifier(random_state=0, max_depth=3)
    clf.fit(X, y)
    score = clf.score(X, y)

    def reduce_ndarray(arr):
        return arr.byteswap().view(arr.dtype.newbyteorder()).__reduce__()

    def get_pickle_non_native_endianness():
        f = io.BytesIO()
        p = pickle.Pickler(f)
        p.dispatch_table = copyreg.dispatch_table.copy()
        p.dispatch_table[np.ndarray] = reduce_ndarray

        p.dump(clf)
        f.seek(0)
        return f

    new_clf = pickle.load(get_pickle_non_native_endianness())
    new_score = new_clf.score(X, y)
    assert np.isclose(score, new_score)


def test_different_endianness_joblib_pickle():
    X, y = datasets.make_classification(random_state=0)

    clf = DecisionTreeClassifier(random_state=0, max_depth=3)
    clf.fit(X, y)
    score = clf.score(X, y)

    class NonNativeEndiannessNumpyPickler(NumpyPickler):
        def save(self, obj):
            if isinstance(obj, np.ndarray):
                obj = obj.byteswap().view(obj.dtype.newbyteorder())
            super().save(obj)

    def get_joblib_pickle_non_native_endianness():
        f = io.BytesIO()
        p = NonNativeEndiannessNumpyPickler(f)

        p.dump(clf)
        f.seek(0)
        return f

    new_clf = joblib.load(get_joblib_pickle_non_native_endianness())
    new_score = new_clf.score(X, y)
    assert np.isclose(score, new_score)


def get_different_bitness_node_ndarray(node_ndarray):
    new_dtype_for_indexing_fields = np.int64 if _IS_32BIT else np.int32

    # field names in Node struct with SIZE_t types (see sklearn/tree/_tree.pxd)
    indexing_field_names = ["left_child", "right_child", "feature", "n_node_samples"]

    new_dtype_dict = {
        name: dtype for name, (dtype, _) in node_ndarray.dtype.fields.items()
    }
    for name in indexing_field_names:
        new_dtype_dict[name] = new_dtype_for_indexing_fields

    new_dtype = np.dtype(
        {"names": list(new_dtype_dict.keys()), "formats": list(new_dtype_dict.values())}
    )
    return node_ndarray.astype(new_dtype, casting="same_kind")


def get_different_alignment_node_ndarray(node_ndarray):
    new_dtype_dict = {
        name: dtype for name, (dtype, _) in node_ndarray.dtype.fields.items()
    }
    offsets = [offset for dtype, offset in node_ndarray.dtype.fields.values()]
    shifted_offsets = [8 + offset for offset in offsets]

    new_dtype = np.dtype(
        {
            "names": list(new_dtype_dict.keys()),
            "formats": list(new_dtype_dict.values()),
            "offsets": shifted_offsets,
        }
    )
    return node_ndarray.astype(new_dtype, casting="same_kind")


def reduce_tree_with_different_bitness(tree):
    new_dtype = np.int64 if _IS_32BIT else np.int32
    tree_cls, (n_features, n_classes, n_outputs), state = tree.__reduce__()
    new_n_classes = n_classes.astype(new_dtype, casting="same_kind")

    new_state = state.copy()
    new_state["nodes"] = get_different_bitness_node_ndarray(new_state["nodes"])

    return (tree_cls, (n_features, new_n_classes, n_outputs), new_state)


def test_different_bitness_pickle():
    X, y = datasets.make_classification(random_state=0)

    clf = DecisionTreeClassifier(random_state=0, max_depth=3)
    clf.fit(X, y)
    score = clf.score(X, y)

    def pickle_dump_with_different_bitness():
        f = io.BytesIO()
        p = pickle.Pickler(f)
        p.dispatch_table = copyreg.dispatch_table.copy()
        p.dispatch_table[CythonTree] = reduce_tree_with_different_bitness

        p.dump(clf)
        f.seek(0)
        return f

    new_clf = pickle.load(pickle_dump_with_different_bitness())
    new_score = new_clf.score(X, y)
    assert score == pytest.approx(new_score)


def test_different_bitness_joblib_pickle():
    # Make sure that a platform specific pickle generated on a 64 bit
    # platform can be converted at pickle load time into an estimator
    # with Cython code that works with the host's native integer precision
    # to index nodes in the tree data structure when the host is a 32 bit
    # platform (and vice versa).
    X, y = datasets.make_classification(random_state=0)

    clf = DecisionTreeClassifier(random_state=0, max_depth=3)
    clf.fit(X, y)
    score = clf.score(X, y)

    def joblib_dump_with_different_bitness():
        f = io.BytesIO()
        p = NumpyPickler(f)
        p.dispatch_table = copyreg.dispatch_table.copy()
        p.dispatch_table[CythonTree] = reduce_tree_with_different_bitness

        p.dump(clf)
        f.seek(0)
        return f

    new_clf = joblib.load(joblib_dump_with_different_bitness())
    new_score = new_clf.score(X, y)
    assert score == pytest.approx(new_score)


def test_check_n_classes():
    expected_dtype = np.dtype(np.int32) if _IS_32BIT else np.dtype(np.int64)
    allowed_dtypes = [np.dtype(np.int32), np.dtype(np.int64)]
    allowed_dtypes += [dt.newbyteorder() for dt in allowed_dtypes]

    n_classes = np.array([0, 1], dtype=expected_dtype)
    for dt in allowed_dtypes:
        _check_n_classes(n_classes.astype(dt), expected_dtype)

    with pytest.raises(ValueError, match="Wrong dimensions.+n_classes"):
        wrong_dim_n_classes = np.array([[0, 1]], dtype=expected_dtype)
        _check_n_classes(wrong_dim_n_classes, expected_dtype)

    with pytest.raises(ValueError, match="n_classes.+incompatible dtype"):
        wrong_dtype_n_classes = n_classes.astype(np.float64)
        _check_n_classes(wrong_dtype_n_classes, expected_dtype)


def test_check_value_ndarray():
    expected_dtype = np.dtype(np.float64)
    expected_shape = (5, 1, 2)
    value_ndarray = np.zeros(expected_shape, dtype=expected_dtype)

    allowed_dtypes = [expected_dtype, expected_dtype.newbyteorder()]

    for dt in allowed_dtypes:
        _check_value_ndarray(
            value_ndarray, expected_dtype=dt, expected_shape=expected_shape
        )

    with pytest.raises(ValueError, match="Wrong shape.+value array"):
        _check_value_ndarray(
            value_ndarray, expected_dtype=expected_dtype, expected_shape=(1, 2)
        )

    for problematic_arr in [value_ndarray[:, :, :1], np.asfortranarray(value_ndarray)]:
        with pytest.raises(ValueError, match="value array.+C-contiguous"):
            _check_value_ndarray(
                problematic_arr,
                expected_dtype=expected_dtype,
                expected_shape=problematic_arr.shape,
            )

    with pytest.raises(ValueError, match="value array.+incompatible dtype"):
        _check_value_ndarray(
            value_ndarray.astype(np.float32),
            expected_dtype=expected_dtype,
            expected_shape=expected_shape,
        )


def test_check_node_ndarray():
    expected_dtype = NODE_DTYPE

    node_ndarray = np.zeros((5,), dtype=expected_dtype)

    valid_node_ndarrays = [
        node_ndarray,
        get_different_bitness_node_ndarray(node_ndarray),
        get_different_alignment_node_ndarray(node_ndarray),
    ]
    valid_node_ndarrays += [
        arr.astype(arr.dtype.newbyteorder()) for arr in valid_node_ndarrays
    ]

    for arr in valid_node_ndarrays:
        _check_node_ndarray(node_ndarray, expected_dtype=expected_dtype)

    with pytest.raises(ValueError, match="Wrong dimensions.+node array"):
        problematic_node_ndarray = np.zeros((5, 2), dtype=expected_dtype)
        _check_node_ndarray(problematic_node_ndarray, expected_dtype=expected_dtype)

    with pytest.raises(ValueError, match="node array.+C-contiguous"):
        problematic_node_ndarray = node_ndarray[::2]
        _check_node_ndarray(problematic_node_ndarray, expected_dtype=expected_dtype)

    dtype_dict = {name: dtype for name, (dtype, _) in node_ndarray.dtype.fields.items()}

    # array with wrong 'threshold' field dtype (int64 rather than float64)
    new_dtype_dict = dtype_dict.copy()
    new_dtype_dict["threshold"] = np.int64

    new_dtype = np.dtype(
        {"names": list(new_dtype_dict.keys()), "formats": list(new_dtype_dict.values())}
    )
    problematic_node_ndarray = node_ndarray.astype(new_dtype)

    with pytest.raises(ValueError, match="node array.+incompatible dtype"):
        _check_node_ndarray(problematic_node_ndarray, expected_dtype=expected_dtype)

    # array with wrong 'left_child' field dtype (float64 rather than int64 or int32)
    new_dtype_dict = dtype_dict.copy()
    new_dtype_dict["left_child"] = np.float64
    new_dtype = np.dtype(
        {"names": list(new_dtype_dict.keys()), "formats": list(new_dtype_dict.values())}
    )

    problematic_node_ndarray = node_ndarray.astype(new_dtype)

    with pytest.raises(ValueError, match="node array.+incompatible dtype"):
        _check_node_ndarray(problematic_node_ndarray, expected_dtype=expected_dtype)


@pytest.mark.parametrize(
    "Splitter", chain(DENSE_SPLITTERS.values(), SPARSE_SPLITTERS.values())
)
def test_splitter_serializable(Splitter):
    """Check that splitters are serializable."""
    rng = np.random.RandomState(42)
    max_features = 10
    n_outputs, n_classes = 2, np.array([3, 2], dtype=np.intp)

    criterion = CRITERIA_CLF["gini"](n_outputs, n_classes)
    splitter = Splitter(criterion, max_features, 5, 0.5, rng, monotonic_cst=None)
    splitter_serialize = pickle.dumps(splitter)

    splitter_back = pickle.loads(splitter_serialize)
    assert splitter_back.max_features == max_features
    assert isinstance(splitter_back, Splitter)


def test_tree_deserialization_from_read_only_buffer(tmpdir):
    """Check that Trees can be deserialized with read only buffers.

    Non-regression test for gh-25584.
    """
    pickle_path = str(tmpdir.join("clf.joblib"))
    clf = DecisionTreeClassifier(random_state=0)
    clf.fit(X_small, y_small)

    joblib.dump(clf, pickle_path)
    loaded_clf = joblib.load(pickle_path, mmap_mode="r")

    assert_tree_equal(
        loaded_clf.tree_,
        clf.tree_,
        "The trees of the original and loaded classifiers are not equal.",
    )


@pytest.mark.parametrize("Tree", ALL_TREES.values())
def test_min_sample_split_1_error(Tree):
    """Check that an error is raised when min_sample_split=1.

    non-regression test for issue gh-25481.
    """
    X = np.array([[0, 0], [1, 1]])
    y = np.array([0, 1])

    # min_samples_split=1.0 is valid
    Tree(min_samples_split=1.0).fit(X, y)

    # min_samples_split=1 is invalid
    tree = Tree(min_samples_split=1)
    msg = (
        r"'min_samples_split' .* must be an int in the range \[2, inf\) "
        r"or a float in the range \(0.0, 1.0\]"
    )
    with pytest.raises(ValueError, match=msg):
        tree.fit(X, y)


@pytest.mark.parametrize("criterion", ["squared_error", "friedman_mse"])
def test_missing_values_on_equal_nodes_no_missing(criterion):
    """Check missing values goes to correct node during predictions"""
    X = np.array([[0, 1, 2, 3, 8, 9, 11, 12, 15]]).T
    y = np.array([0.1, 0.2, 0.3, 0.2, 1.4, 1.4, 1.5, 1.6, 2.6])

    dtc = DecisionTreeRegressor(random_state=42, max_depth=1, criterion=criterion)
    dtc.fit(X, y)

    # Goes to right node because it has the most data points
    y_pred = dtc.predict([[np.nan]])
    assert_allclose(y_pred, [np.mean(y[-5:])])

    # equal number of elements in both nodes
    X_equal = X[:-1]
    y_equal = y[:-1]

    dtc = DecisionTreeRegressor(random_state=42, max_depth=1, criterion=criterion)
    dtc.fit(X_equal, y_equal)

    # Goes to right node because the implementation sets:
    # missing_go_to_left = n_left > n_right, which is False
    y_pred = dtc.predict([[np.nan]])
    assert_allclose(y_pred, [np.mean(y_equal[-4:])])


@pytest.mark.parametrize("criterion", ["entropy", "gini"])
def test_missing_values_best_splitter_three_classes(criterion):
    """Test when missing values are uniquely present in a class among 3 classes."""
    missing_values_class = 0
    X = np.array([[np.nan] * 4 + [0, 1, 2, 3, 8, 9, 11, 12]]).T
    y = np.array([missing_values_class] * 4 + [1] * 4 + [2] * 4)
    dtc = DecisionTreeClassifier(random_state=42, max_depth=2, criterion=criterion)
    dtc.fit(X, y)

    X_test = np.array([[np.nan, 3, 12]]).T
    y_nan_pred = dtc.predict(X_test)
    # Missing values necessarily are associated to the observed class.
    assert_array_equal(y_nan_pred, [missing_values_class, 1, 2])


@pytest.mark.parametrize("criterion", ["entropy", "gini"])
def test_missing_values_best_splitter_to_left(criterion):
    """Missing values spanning only one class at fit-time must make missing
    values at predict-time be classified has belonging to this class."""
    X = np.array([[np.nan] * 4 + [0, 1, 2, 3, 4, 5]]).T
    y = np.array([0] * 4 + [1] * 6)

    dtc = DecisionTreeClassifier(random_state=42, max_depth=2, criterion=criterion)
    dtc.fit(X, y)

    X_test = np.array([[np.nan, 5, np.nan]]).T
    y_pred = dtc.predict(X_test)

    assert_array_equal(y_pred, [0, 1, 0])


@pytest.mark.parametrize("criterion", ["entropy", "gini"])
def test_missing_values_best_splitter_to_right(criterion):
    """Missing values and non-missing values sharing one class at fit-time
    must make missing values at predict-time be classified has belonging
    to this class."""
    X = np.array([[np.nan] * 4 + [0, 1, 2, 3, 4, 5]]).T
    y = np.array([1] * 4 + [0] * 4 + [1] * 2)

    dtc = DecisionTreeClassifier(random_state=42, max_depth=2, criterion=criterion)
    dtc.fit(X, y)

    X_test = np.array([[np.nan, 1.2, 4.8]]).T
    y_pred = dtc.predict(X_test)

    assert_array_equal(y_pred, [1, 0, 1])


@pytest.mark.parametrize("criterion", ["entropy", "gini"])
def test_missing_values_missing_both_classes_has_nan(criterion):
    """Check behavior of missing value when there is one missing value in each class."""
    X = np.array([[1, 2, 3, 5, np.nan, 10, 20, 30, 60, np.nan]]).T
    y = np.array([0] * 5 + [1] * 5)

    dtc = DecisionTreeClassifier(random_state=42, max_depth=1, criterion=criterion)
    dtc.fit(X, y)
    X_test = np.array([[np.nan, 2.3, 34.2]]).T
    y_pred = dtc.predict(X_test)

    # Missing value goes to the class at the right (here 1) because the implementation
    # searches right first.
    assert_array_equal(y_pred, [1, 0, 1])


@pytest.mark.parametrize("sparse_container", [None] + CSR_CONTAINERS)
@pytest.mark.parametrize(
    "tree",
    [
        DecisionTreeClassifier(splitter="random"),
        DecisionTreeRegressor(criterion="absolute_error"),
    ],
)
def test_missing_value_errors(sparse_container, tree):
    """Check unsupported configurations for missing values."""

    X = np.array([[1, 2, 3, 5, np.nan, 10, 20, 30, 60, np.nan]]).T
    y = np.array([0] * 5 + [1] * 5)

    if sparse_container is not None:
        X = sparse_container(X)

    with pytest.raises(ValueError, match="Input X contains NaN"):
        tree.fit(X, y)


def test_missing_values_poisson():
    """Smoke test for poisson regression and missing values."""
    X, y = diabetes.data.copy(), diabetes.target

    # Set some values missing
    X[::5, 0] = np.nan
    X[::6, -1] = np.nan

    reg = DecisionTreeRegressor(criterion="poisson", random_state=42)
    reg.fit(X, y)

    y_pred = reg.predict(X)
    assert (y_pred >= 0.0).all()


def make_friedman1_classification(*args, **kwargs):
    X, y = datasets.make_friedman1(*args, **kwargs)
    y = y > 14
    return X, y


@pytest.mark.parametrize(
    "make_data,Tree",
    [
        (datasets.make_friedman1, DecisionTreeRegressor),
        (make_friedman1_classification, DecisionTreeClassifier),
    ],
)
@pytest.mark.parametrize("sample_weight_train", [None, "ones"])
def test_missing_values_is_resilience(
    make_data, Tree, sample_weight_train, global_random_seed
):
    """Check that trees can deal with missing values have decent performance."""
    n_samples, n_features = 5_000, 10
    X, y = make_data(
        n_samples=n_samples, n_features=n_features, random_state=global_random_seed
    )

    X_missing = X.copy()
    rng = np.random.RandomState(global_random_seed)
    X_missing[rng.choice([False, True], size=X.shape, p=[0.9, 0.1])] = np.nan
    X_missing_train, X_missing_test, y_train, y_test = train_test_split(
        X_missing, y, random_state=global_random_seed
    )
    if sample_weight_train == "ones":
        sample_weight = np.ones(X_missing_train.shape[0])
    else:
        sample_weight = None

    native_tree = Tree(max_depth=10, random_state=global_random_seed)
    native_tree.fit(X_missing_train, y_train, sample_weight=sample_weight)
    score_native_tree = native_tree.score(X_missing_test, y_test)

    tree_with_imputer = make_pipeline(
        SimpleImputer(), Tree(max_depth=10, random_state=global_random_seed)
    )
    tree_with_imputer.fit(X_missing_train, y_train)
    score_tree_with_imputer = tree_with_imputer.score(X_missing_test, y_test)

    assert (
        score_native_tree > score_tree_with_imputer
    ), f"{score_native_tree=} should be strictly greater than {score_tree_with_imputer}"


def test_missing_value_is_predictive():
    """Check the tree learns when only the missing value is predictive."""
    rng = np.random.RandomState(0)
    n_samples = 1000

    X = rng.standard_normal(size=(n_samples, 10))
    y = rng.randint(0, high=2, size=n_samples)

    # Create a predictive feature using `y` and with some noise
    X_random_mask = rng.choice([False, True], size=n_samples, p=[0.95, 0.05])
    y_mask = y.copy().astype(bool)
    y_mask[X_random_mask] = ~y_mask[X_random_mask]

    X_predictive = rng.standard_normal(size=n_samples)
    X_predictive[y_mask] = np.nan

    X[:, 5] = X_predictive

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)
    tree = DecisionTreeClassifier(random_state=rng).fit(X_train, y_train)

    assert tree.score(X_train, y_train) >= 0.85
    assert tree.score(X_test, y_test) >= 0.85


@pytest.mark.parametrize(
    "make_data, Tree",
    [
        (datasets.make_regression, DecisionTreeRegressor),
        (datasets.make_classification, DecisionTreeClassifier),
    ],
)
def test_sample_weight_non_uniform(make_data, Tree):
    """Check sample weight is correctly handled with missing values."""
    rng = np.random.RandomState(0)
    n_samples, n_features = 1000, 10
    X, y = make_data(n_samples=n_samples, n_features=n_features, random_state=rng)

    # Create dataset with missing values
    X[rng.choice([False, True], size=X.shape, p=[0.9, 0.1])] = np.nan

    # Zero sample weight is the same as removing the sample
    sample_weight = np.ones(X.shape[0])
    sample_weight[::2] = 0.0

    tree_with_sw = Tree(random_state=0)
    tree_with_sw.fit(X, y, sample_weight=sample_weight)

    tree_samples_removed = Tree(random_state=0)
    tree_samples_removed.fit(X[1::2, :], y[1::2])

    assert_allclose(tree_samples_removed.predict(X), tree_with_sw.predict(X))


def test_deterministic_pickle():
    # Non-regression test for:
    # https://github.com/scikit-learn/scikit-learn/issues/27268
    # Uninitialised memory would lead to the two pickle strings being different.
    tree1 = DecisionTreeClassifier(random_state=0).fit(iris.data, iris.target)
    tree2 = DecisionTreeClassifier(random_state=0).fit(iris.data, iris.target)

    pickle1 = pickle.dumps(tree1)
    pickle2 = pickle.dumps(tree2)

    assert pickle1 == pickle2


@pytest.mark.parametrize(
    "X",
    [
        # missing values will go left for greedy splits
        np.array([np.nan, 2, np.nan, 4, 5, 6]),
        np.array([np.nan, np.nan, 3, 4, 5, 6]),
        # missing values will go right for greedy splits
        np.array([1, 2, 3, 4, np.nan, np.nan]),
        np.array([1, 2, 3, np.nan, 6, np.nan]),
    ],
)
@pytest.mark.parametrize("criterion", ["squared_error", "friedman_mse"])
def test_regression_tree_missing_values_toy(X, criterion):
    """Check that we properly handle missing values in regression trees using a toy
    dataset.

    The regression targeted by this test was that we were not reinitializing the
    criterion when it comes to the number of missing values. Therefore, the value
    of the critetion (i.e. MSE) was completely wrong.

    This test check that the MSE is null when there is a single sample in the leaf.

    Non-regression test for:
    https://github.com/scikit-learn/scikit-learn/issues/28254
    https://github.com/scikit-learn/scikit-learn/issues/28316
    """
    X = X.reshape(-1, 1)
    y = np.arange(6)

    tree = DecisionTreeRegressor(criterion=criterion, random_state=0).fit(X, y)
    tree_ref = clone(tree).fit(y.reshape(-1, 1), y)
    assert all(tree.tree_.impurity >= 0)  # MSE should always be positive
    # Check the impurity match after the first split
    assert_allclose(tree.tree_.impurity[:2], tree_ref.tree_.impurity[:2])

    # Find the leaves with a single sample where the MSE should be 0
    leaves_idx = np.flatnonzero(
        (tree.tree_.children_left == -1) & (tree.tree_.n_node_samples == 1)
    )
    assert_allclose(tree.tree_.impurity[leaves_idx], 0.0)


def test_classification_tree_missing_values_toy():
    """Check that we properly handle missing values in clasification trees using a toy
    dataset.

    The test is more involved because we use a case where we detected a regression
    in a random forest. We therefore define the seed and bootstrap indices to detect
    one of the non-frequent regression.

    Here, we check that the impurity is null or positive in the leaves.

    Non-regression test for:
    https://github.com/scikit-learn/scikit-learn/issues/28254
    """
    X, y = datasets.load_iris(return_X_y=True)

    rng = np.random.RandomState(42)
    X_missing = X.copy()
    mask = rng.binomial(
        n=np.ones(shape=(1, 4), dtype=np.int32), p=X[:, [2]] / 8
    ).astype(bool)
    X_missing[mask] = np.nan
    X_train, _, y_train, _ = train_test_split(X_missing, y, random_state=13)

    # fmt: off
    # no black reformatting for this specific array
    indices = np.array([
        2, 81, 39, 97, 91, 38, 46, 31, 101, 13, 89, 82, 100, 42, 69, 27, 81, 16, 73, 74,
        51, 47, 107, 17, 75, 110, 20, 15, 104, 57, 26, 15, 75, 79, 35, 77, 90, 51, 46,
        13, 94, 91, 23, 8, 93, 93, 73, 77, 12, 13, 74, 109, 110, 24, 10, 23, 104, 27,
        92, 52, 20, 109, 8, 8, 28, 27, 35, 12, 12, 7, 43, 0, 30, 31, 78, 12, 24, 105,
        50, 0, 73, 12, 102, 105, 13, 31, 1, 69, 11, 32, 75, 90, 106, 94, 60, 56, 35, 17,
        62, 85, 81, 39, 80, 16, 63, 6, 80, 84, 3, 3, 76, 78
    ], dtype=np.int32)
    # fmt: on

    tree = DecisionTreeClassifier(
        max_depth=3, max_features="sqrt", random_state=1857819720
    )
    tree.fit(X_train[indices], y_train[indices])
    assert all(tree.tree_.impurity >= 0)

    leaves_idx = np.flatnonzero(
        (tree.tree_.children_left == -1) & (tree.tree_.n_node_samples == 1)
    )
    assert_allclose(tree.tree_.impurity[leaves_idx], 0.0)

def test_negative_expected_value_tree_sample_data():
    #no constraints initially
    tree = ExpectedValueDecisionTreeRegressor(
        splitter="best",
        max_depth=1, random_state=1857819720
    )
    
    #rng = np.random.RandomState(0)
    #num_prices = 100
    #prices = np.linspace(5, 25, num_prices)
    #num_samples = 10

    #slopes = rng.uniform(low=0.8, high=2, size=num_samples)
    #shifts = rng.uniform(low = 5, high = 10, size = num_samples)
    #y = np.array([1/(1+np.exp(slopes[i]*(prices - shifts[i]))) for i in range(len(slopes))])

    #slopes = rng.uniform(low=10000, high=400000, size=num_samples)
    #deal_values = np.array([slopes[i]*prices for i in range(len(slopes))])
    #deal_volumes = np.array([max(deal_values[i]) for i in range(len(deal_values))])
    
    #ev = np.array([y[i]*deal_values[i] for i in range(len(y))])
    #optimals = np.argmax(ev, axis=1)
    #median = np.median(optimals)
    #X = np.where(np.argmax(ev, axis=1) < median, 0, 1).reshape(len(y), -1)

    deal_values = deal_value
    deal_volumes = deal_volume
    prices = prices_value
    X= X_value
    y = y_value

    #print(f"{y.shape} y shape")
    #print(f"{X.shape} X shape")
    #print(f"{deal_values.shape} deal_values shape")
    #print(f"{deal_volumes.shape} deal_volumes shape")
    #print(f"{prices.shape} prices shape")
    tree.fit(X, y, deal_values, deal_volumes, prices)
    #print(tree.tree_.value)
    #print(tree.get_depth())
    #print(tree.get_n_leaves())
    assert tree.get_depth() == 1
    assert tree.get_n_leaves() == 2
    assert np.isclose(tree.tree_.value[1], 4.848484848484849, 0.001) # left node value
    assert np.isclose(tree.tree_.value[2], 7.6767676767676765, 0.001) # right node value

def test_negative_expected_value_tree_sample_data_with_vol_cst():
    #no constraints initially
    tree = ExpectedValueDecisionTreeRegressor(
        splitter="best",
        max_depth=1, random_state=1857819720,
        volume_cst = 0.8 # have to win at least 80% of the original volume throughout the leaves
    )

    deal_values = deal_value
    deal_volumes = deal_volume
    prices = prices_value
    X= X_value
    y = y_value
    tree.fit(X, y, deal_values, deal_volumes, prices)
    assert tree.get_depth() == 1
    assert tree.get_n_leaves() == 2
    assert np.isclose(tree.tree_.value[1], 4.848484848484849, 0.001) # left node value
    assert np.isclose(tree.tree_.value[2], 7.6767676767676765, 0.001) # right node value

def test_negative_expected_value_tree_sample_data_with_vol_cst_too_strict():
    #no constraints initially
    tree = ExpectedValueDecisionTreeRegressor(
        splitter="best",
        max_depth=1, random_state=1857819720,
        volume_cst = 0.90 # have to win at least 90% of the original volume which is not possible with these features
    )

    deal_values = deal_value
    deal_volumes = deal_volume
    prices = prices_value
    X= X_value
    y = y_value
    tree.fit(X, y, deal_values, deal_volumes, prices)
    assert tree.get_depth() == 0
    assert tree.get_n_leaves() == 1








